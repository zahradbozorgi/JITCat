{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5a683265-5556-4ea9-b537-477150acee41",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import seaborn as sns\n",
    "\n",
    "from pathlib import Path\n",
    "import os\n",
    "import time\n",
    "import itertools\n",
    "import argparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fc01ccd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from catboost import CatBoostClassifier\n",
    "import sklearn \n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, f1_score, precision_score, recall_score\n",
    "from sklearn.metrics import classification_report, confusion_matrix, auc, roc_curve\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler, PolynomialFeatures, LabelEncoder\n",
    "from sklearn.pipeline import make_pipeline, Pipeline\n",
    "from sklearn.compose import make_column_selector, make_column_transformer\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "from sklearn.frozen import FrozenEstimator\n",
    "# from sklearn.preprocessing import OneHotEncoder, OrdinalEncoder\n",
    "\n",
    "# import math\n",
    "# from statsmodels.tsa.arima.model import ARIMA\n",
    "# from sklearn.model_selection import ParameterGrid\n",
    "# from joblib import Parallel, delayed\n",
    "# import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ac719a1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "ROOT_DIR = Path().cwd()\n",
    "while not ROOT_DIR.joinpath(\"data\").exists():\n",
    "    ROOT_DIR = ROOT_DIR.parent\n",
    "os.chdir(ROOT_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8ccb520c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bpic2011_f1\n",
      "bpic2011_f2\n",
      "bpic2011_f3\n",
      "bpic2011_f4\n"
     ]
    }
   ],
   "source": [
    "from processors import processor_factory\n",
    "from data.DatasetManager import DatasetManager\n",
    "import data.EncoderFactory as EncoderFactory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d6efa3ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import category_encoders as ce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c4292a19",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5331955e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_first_n_cases(df, n, dataset_manager):\n",
    "    earliest_timestamps = df.groupby(dataset_manager.case_id_col)[dataset_manager.timestamp_col].min()\n",
    "    sorted_cases = earliest_timestamps.sort_values().index[:n]\n",
    "    return df[df[dataset_manager.case_id_col].isin(sorted_cases)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "79f76a6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_last_row(group):\n",
    "    max_event_row = group.loc[group['event_nr'].idxmax()]\n",
    "    return max_event_row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ded07b80",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_threshold(proba_values, true_values):\n",
    "    list_acc = []\n",
    "    thresholds = np.arange(0, 1.0, 0.05)\n",
    "    true_values = true_values.map({'regular': True, 'deviant': False})\n",
    "\n",
    "\n",
    "    for threshold in thresholds:\n",
    "        preds_thr = proba_values > threshold\n",
    "        acc= accuracy_score(true_values, preds_thr)\n",
    "        # acc = np.mean(true_values == preds_thr)\n",
    "        list_acc.append(acc)\n",
    "        \n",
    "        #print(f\"Threshold: {threshold}, Accuracy: {acc}\")\n",
    "\n",
    "    # plt.plot(thresholds, list_acc)\n",
    "\n",
    "\n",
    "    best_threshold = thresholds[np.where(list_acc==np.max(list_acc))]\n",
    "    best_accuracy = np.max(list_acc)\n",
    "\n",
    "    # print(f\"Best Threshold: {best_threshold}, Best Accuracy: {best_accuracy}\")\n",
    "    return best_threshold, best_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "25f491fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_moving_avg_f1(df, true_col, pred_col, dataset_manager, window_size=5):\n",
    "    # Sort the DataFrame by the timestamp column\n",
    "    df = df.sort_values([dataset_manager.timestamp_col], ascending=True, kind='mergesort')\n",
    "\n",
    "    # Convert columns to numpy arrays for faster operations\n",
    "    true_values = df[true_col].to_numpy()\n",
    "    predicted_values = df[pred_col].to_numpy()\n",
    "\n",
    "    # Preallocate arrays for results\n",
    "    num_rows = np.arange(2, len(true_values) + 1)  # Start from 2\n",
    "    f1_list = np.zeros(len(num_rows))\n",
    "\n",
    "    # Compute F1 scores incrementally\n",
    "    for i in range(2, len(true_values) + 1):\n",
    "        f1_list[i - 2] = f1_score(true_values[:i], predicted_values[:i], average='weighted')\n",
    "\n",
    "    # Create a DataFrame for results\n",
    "    f1_df = pd.DataFrame({'num_rows': num_rows, 'f1': f1_list})\n",
    "\n",
    "    # Compute the moving average of F1 scores\n",
    "    f1_df['moving_avg_f1'] = f1_df['f1'].rolling(window=window_size, min_periods=1).mean()\n",
    "\n",
    "    return f1_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "411597d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_moving_avg_acc_fast(df, true_col, pred_col, dataset_manager, window_size=5):\n",
    "\n",
    "    df.sort_values([dataset_manager.timestamp_col], ascending=True, kind='mergesort', inplace=True)\n",
    "    # Convert columns to numpy arrays\n",
    "    true_values = df[true_col].to_numpy()\n",
    "    predicted_values = df[pred_col].to_numpy()\n",
    "\n",
    "\n",
    "    # Calculate cumulative accuracy (vectorized)\n",
    "    cumulative_correct = np.cumsum(true_values == predicted_values)\n",
    "    num_rows = np.arange(1, len(true_values) + 1)\n",
    "    cumulative_accuracy = cumulative_correct / num_rows\n",
    "\n",
    "    # Create DataFrame for results\n",
    "    mae_df = pd.DataFrame({\n",
    "        'num_rows': num_rows[1:],  # Start from 2\n",
    "        'mae': cumulative_accuracy[1:]  # Start from 2\n",
    "    })\n",
    "\n",
    "    # Calculate moving average of MAE\n",
    "    mae_df['moving_avg_mae'] = mae_df['mae'].rolling(window=window_size).mean()\n",
    "\n",
    "    return mae_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "acd52c86",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_results_to_csv(dataset_name, proposed_metrics, baseline_metrics, file_path='results_metrics_HP_tuning.csv'):\n",
    "    \"\"\"\n",
    "    Save results metrics to a CSV file.\n",
    "\n",
    "    Parameters:\n",
    "        dataset_name (str): Name of the dataset.\n",
    "        proposed_metrics (dict): Metrics for the proposed method (keys: accuracy, f1_score, auc, precision, recall).\n",
    "        baseline_metrics (dict): Metrics for the baseline method (keys: accuracy, f1_score, auc, precision, recall).\n",
    "        file_path (str): Path to the results CSV file.\n",
    "    \"\"\"\n",
    "    # Define the columns for the CSV file\n",
    "    columns = [\n",
    "        'dataset_name',\n",
    "        'proposed_accuracy', 'proposed_f1_score', 'proposed_auc', 'proposed_precision', 'proposed_recall',\n",
    "        'baseline_accuracy', 'baseline_f1_score', 'baseline_auc', 'baseline_precision', 'baseline_recall'\n",
    "    ]\n",
    "\n",
    "    # Create a DataFrame for the new results\n",
    "    new_data = {\n",
    "        'dataset_name': dataset_name,\n",
    "        'proposed_accuracy': proposed_metrics['accuracy'],\n",
    "        'proposed_f1_score': proposed_metrics['f1_score'],\n",
    "        'proposed_auc': proposed_metrics['auc'],\n",
    "        'proposed_precision': proposed_metrics['precision'],\n",
    "        'proposed_recall': proposed_metrics['recall'],\n",
    "        'baseline_accuracy': baseline_metrics['accuracy'],\n",
    "        'baseline_f1_score': baseline_metrics['f1_score'],\n",
    "        'baseline_auc': baseline_metrics['auc'],\n",
    "        'baseline_precision': baseline_metrics['precision'],\n",
    "        'baseline_recall': baseline_metrics['recall']\n",
    "    }\n",
    "\n",
    "    new_row = pd.DataFrame([new_data])\n",
    "\n",
    "    # Check if the file exists\n",
    "    if not os.path.exists(file_path):\n",
    "        # If the file doesn't exist, create it with the appropriate headers\n",
    "        new_row.to_csv(file_path, index=False, columns=columns)\n",
    "    else:\n",
    "        # If the file exists, append the new row\n",
    "        existing_data = pd.read_csv(file_path)\n",
    "        # Check if the dataset already exists in the file\n",
    "        if dataset_name in existing_data['dataset_name'].values:\n",
    "            print(f\"Dataset '{dataset_name}' already exists in the results file. Updating the row.\")\n",
    "            \n",
    "            # Remove the existing row with the same dataset_name\n",
    "            existing_data = existing_data[existing_data['dataset_name'] != dataset_name]\n",
    "            \n",
    "            # Append the new row to the DataFrame\n",
    "            updated_data = pd.concat([existing_data, new_row], ignore_index=True)\n",
    "            \n",
    "            # Overwrite the file with the updated DataFrame\n",
    "            updated_data.to_csv(file_path, mode='w', index=False, columns=columns)\n",
    "        else:\n",
    "            new_row.to_csv(file_path, mode='a', index=False, header=False, columns=columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "69a4fe56",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_metrics(results_df, dataset_manager):\n",
    "    \"\"\"\n",
    "    Calculate metrics from a results DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "        results_df (pd.DataFrame): DataFrame containing 'true_labels', 'predicted_labels', and 'predicted_probs'.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary containing accuracy, f1_score, auc, precision, and recall.\n",
    "    \"\"\"\n",
    "    true_labels = results_df[dataset_manager.label_col]\n",
    "    predicted_labels = results_df['predicted_value']\n",
    "    predicted_probs = results_df['proba_of_regular']\n",
    "\n",
    "    metrics = {\n",
    "        'accuracy': accuracy_score(true_labels, predicted_labels),\n",
    "        'f1_score': f1_score(true_labels, predicted_labels, average='weighted'),\n",
    "        'auc': roc_auc_score(true_labels, predicted_probs),\n",
    "        'precision': precision_score(true_labels, predicted_labels, average='weighted'),\n",
    "        'recall': recall_score(true_labels, predicted_labels, average='weighted')\n",
    "    }\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fd096c8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing dataset: production\n",
      "Test mode: False\n"
     ]
    }
   ],
   "source": [
    "dataset_name = 'production'\n",
    "test_mode = False\n",
    "\n",
    "# # Parse command-line arguments\n",
    "# parser = argparse.ArgumentParser(description=\"Process dataset name and test mode as arguments.\")\n",
    "# parser.add_argument('--dataset_name', type=str, required=True, help=\"Name of the dataset to process.\")\n",
    "# parser.add_argument('--test_mode', type=bool, default=False, help=\"Run in test mode (True or False).\")\n",
    "# args = parser.parse_args()\n",
    "\n",
    "# # Use the arguments in your code\n",
    "# dataset_name = args.dataset_name\n",
    "# test_mode = args.test_mode\n",
    "\n",
    "print(f\"Processing dataset: {dataset_name}\")\n",
    "print(f\"Test mode: {test_mode}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0cf377a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_manager = DatasetManager(dataset_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cb06a6a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = dataset_manager.read_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9634f96e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sort_values([dataset_manager.case_id_col, dataset_manager.timestamp_col], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4f42ed2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_case_num = df[dataset_manager.case_id_col].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "222f4d89",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df = get_first_n_cases(df, max_case_num, dataset_manager)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8b0f76a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in [dataset_manager.activity_col]:\n",
    "        counts = filtered_df[col].value_counts()\n",
    "        mask = filtered_df[col].isin(counts[counts >= 100].index)\n",
    "        filtered_df.loc[~mask, col] = \"other\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3b173b67",
   "metadata": {},
   "outputs": [],
   "source": [
    "use_encoding = False\n",
    "use_bucketing = False\n",
    "num_nearest_neighbors = 100\n",
    "distance_metric = 'euclidean'\n",
    "\n",
    "processor = processor_factory.get_processor(dataset_name, use_encoding, use_bucketing, num_nearest_neighbors, distance_metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d3a7130b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# determine min and max (truncated) prefix lengths\n",
    "min_prefix_length = 1\n",
    "if \"traffic_fines\" in dataset_name:\n",
    "    max_prefix_length = 10\n",
    "elif \"bpic2017\" in dataset_name:\n",
    "    max_prefix_length = min(20, dataset_manager.get_pos_case_length_quantile(filtered_df, 0.90))\n",
    "else:\n",
    "    max_prefix_length = min(40, dataset_manager.get_pos_case_length_quantile(filtered_df, 0.90))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e4a2662e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating prefix data...\n",
      "Activity\n"
     ]
    }
   ],
   "source": [
    "start_test_prefix_generation = time.time()\n",
    "print(\"Generating prefix data...\")\n",
    "dt_prefixes = dataset_manager.generate_prefix_data(filtered_df, min_prefix_length, max_prefix_length)\n",
    "test_prefix_generation_time = time.time() - start_test_prefix_generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4f2c296a",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = EncoderFactory.get_encoder(method='agg', case_id_col=dataset_manager.case_id_col, static_cat_cols=dataset_manager.static_cat_cols, \n",
    "                                     static_num_cols=dataset_manager.static_num_cols, dynamic_cat_cols=dataset_manager.dynamic_cat_cols,\n",
    "                                     dynamic_num_cols=dataset_manager.dynamic_num_cols, fillna=True, max_events=None, \n",
    "                                     activity_col=dataset_manager.activity_col, resource_col=None, \n",
    "                                     timestamp_col=dataset_manager.timestamp_col, scale_model=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "27e0164a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_transformed = encoder.transform(dt_prefixes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "00e84c31",
   "metadata": {},
   "outputs": [],
   "source": [
    "subset = dt_prefixes[[dataset_manager.case_id_col, dataset_manager.timestamp_col, dataset_manager.activity_col, dataset_manager.label_col, 'event_nr', 'case_length'] + dataset_manager.static_num_cols+dataset_manager.static_cat_cols]\n",
    "subset = subset.groupby(dataset_manager.case_id_col).apply(return_last_row).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "dae26210",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new column 'finished' with values based on the condition\n",
    "subset['finished'] = (subset['event_nr'] == subset['case_length']).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "da664344",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = pd.merge(subset, dt_transformed, on=[dataset_manager.case_id_col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "bfb4f44a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Identify object columns\n",
    "object_columns = merged_df.select_dtypes(include=['object']).columns\n",
    "\n",
    "# Step 2: Check if object columns contain boolean values\n",
    "for col in object_columns:\n",
    "    if merged_df[col].isin(['True', 'False', 'TRUE', 'FALSE', 'true', 'false']).all():\n",
    "        merged_df[col] = merged_df[col].str.lower().map({'true': True, 'false': False})\n",
    "\n",
    "        # Step 3: Transform boolean object columns to boolean data type\n",
    "        merged_df[col] = merged_df[col].astype('boolean')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "58872e47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter grid established in 0.00 seconds.\n",
      "Data preprocessing completed in 0.01 seconds.\n"
     ]
    }
   ],
   "source": [
    "# Define the hyperparameter grid\n",
    "start = time.time()\n",
    "# Generate all combinations of hyperparameters\n",
    "param_grid = {\n",
    "    'num_nearest_neighbors': [100],\n",
    "    'distance_metric': ['euclidean'],\n",
    "    'encoding_method': ['catboost', 'count', 'onehot', 'target', 'woe', 'rank'],\n",
    "    'model': ['Catboost'],\n",
    "    'batch_size': [100]  # Added batch size as a hyperparameter\n",
    "}\n",
    "param_combinations = list(itertools.product(*param_grid.values()))\n",
    "param_keys = list(param_grid.keys())\n",
    "\n",
    "# Convert combinations to dictionaries\n",
    "param_dicts = [dict(zip(param_keys, values)) for values in param_combinations]\n",
    "end = time.time()\n",
    "print(f\"Parameter grid established in {end - start:.2f} seconds.\")\n",
    "\n",
    "# Preprocess data once\n",
    "start = time.time()\n",
    "data = merged_df.sort_values([dataset_manager.case_id_col, dataset_manager.timestamp_col], ascending=True, kind='mergesort')\n",
    "processor = processor_factory.get_processor(dataset_name, use_encoding=False, use_bucketing=False, \n",
    "            num_nearest_neighbors=num_nearest_neighbors, distance_metric=distance_metric)\n",
    "historic, current = processor.split_data_strict(data, train_ratio=0.5)\n",
    "historic.sort_values([dataset_manager.timestamp_col], ascending=True, kind='mergesort', inplace=True)\n",
    "current.sort_values([dataset_manager.timestamp_col], ascending=True, kind='mergesort', inplace=True)\n",
    "if test_mode:\n",
    "    current = current.head(200)\n",
    "\n",
    "features_used = historic.columns.difference(\n",
    "    [dataset_manager.label_col, dataset_manager.timestamp_col, dataset_manager.case_id_col, 'event_nr', 'case_length', 'finished'], \n",
    "    sort=False\n",
    ")\n",
    "end = time.time()\n",
    "print(f\"Data preprocessing completed in {end - start:.2f} seconds.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ef38eba4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experimenting with baseline version\n",
      "Now training\n",
      "Training time:  0.0043944795926411945  minutes\n",
      "Now predicting\n",
      "Accuracy: 0.6695957820738138\n",
      "f1_score: 0.6614141195825072\n",
      "Training Time: 0.0043944795926411945\n",
      "AUC: 0.778982438659858\n",
      "***********************************\n"
     ]
    }
   ],
   "source": [
    "####### Baseline Version without JITL ########\n",
    "\n",
    "results_baseline = pd.DataFrame()\n",
    "method = 'Catboost'\n",
    "print(f\"Experimenting with baseline version\")\n",
    "\n",
    "target = historic[dataset_manager.label_col].values\n",
    "target_test = current[dataset_manager.label_col]\n",
    "\n",
    "start_time = time.time()\n",
    "if method == 'Catboost':\n",
    "        model = CatBoostClassifier(iterations=100, loss_function='Logloss', eval_metric='AUC', verbose=0, cat_features=[dataset_manager.activity_col]+dataset_manager.static_cat_cols, random_seed=321)\n",
    "        print('Now training')\n",
    "        model.fit(historic[features_used], target, cat_features=[dataset_manager.activity_col]+dataset_manager.static_cat_cols)\n",
    "\n",
    "if method == 'HMM':\n",
    "    # Create an instance of the HMM model\n",
    "    model = hmm.GaussianHMM(n_components=7)  # Specify the number of hidden states\n",
    "    model.fit(historic[features_used])\n",
    "if method == 'LogisticRegression':\n",
    "    from sklearn.preprocessing import OneHotEncoder\n",
    "    from sklearn.pipeline import make_pipeline \n",
    "    from sklearn.compose import make_column_selector, make_column_transformer\n",
    "\n",
    "    preprocessor = make_column_transformer(\n",
    "        (OneHotEncoder(handle_unknown=\"ignore\"), make_column_selector(dtype_include='object')),\n",
    "        remainder='passthrough'\n",
    "    )\n",
    "    model = make_pipeline(\n",
    "        preprocessor,\n",
    "        LogisticRegression()\n",
    "    )\n",
    "    model.fit(X_train[features_used], y_train)\n",
    "\n",
    "end_time = time.time()\n",
    "training_time = (end_time - start_time) / 60\n",
    "print(\"Training time: \", training_time, \" minutes\")\n",
    "# trainingtimes.append(training_time)\n",
    "batch_size = 100\n",
    "\n",
    "print(\"Now predicting\")\n",
    "preds = model.predict(current[features_used])\n",
    "probs = model.predict_proba(current[features_used])[:, 1]\n",
    "y_true = current[dataset_manager.label_col].values\n",
    "auc = roc_auc_score(y_true, probs)\n",
    "\n",
    "results_baseline = current.copy()\n",
    "results_baseline.loc[:, 'predicted_value'] = pd.Series(preds.flatten(), index=current.index)\n",
    "results_baseline.loc[:, 'proba_of_regular'] = pd.Series(probs.flatten(), index=current.index)\n",
    "\n",
    "\n",
    "# Calculate metrics\n",
    "true_values = results_baseline[dataset_manager.label_col]\n",
    "predicted_values = results_baseline['predicted_value']\n",
    "\n",
    "accuracy = np.mean(true_values == predicted_values)\n",
    "\n",
    "# Print metrics\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(f\"f1_score: {f1_score(true_values, predicted_values, average='weighted')}\")\n",
    "print(f\"Training Time: {training_time}\")\n",
    "print(f\"AUC: {auc}\")\n",
    "\n",
    "output_dir = Path(f\"results_HP_tuning/{dataset_name}\")\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "results_baseline.to_csv(f'{output_dir}/baseline_{method}.csv', index=False)\n",
    "print('***********************************')\n",
    "\n",
    "# Calculate metrics for baseline method\n",
    "baseline_metrics = calculate_metrics(results_baseline, dataset_manager)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "33e78d78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ########### for parallel processing - ignore for now ###########\n",
    "\n",
    "# # Define the hyperparameter grid\n",
    "# start = time.time()\n",
    "# param_grid = {\n",
    "#     'num_nearest_neighbors': [100],\n",
    "#     'distance_metric': ['euclidean'],\n",
    "#     'encoding_method': ['quantile', 'onehot'],\n",
    "#     'model': ['Catboost']\n",
    "# }\n",
    "# end = time.time()\n",
    "# print(f\"Parameter grid established in {end - start:.2f} seconds.\")\n",
    "\n",
    "# # Preprocess data once\n",
    "# start = time.time()\n",
    "# data = merged_df.sort_values([dataset_manager.case_id_col, dataset_manager.timestamp_col], ascending=True, kind='mergesort')\n",
    "# processor = processor_factory.get_processor(dataset_name, use_encoding=False, use_bucketing=False, \n",
    "#             num_nearest_neighbors=num_nearest_neighbors, distance_metric=distance_metric)\n",
    "# historic, current = processor.split_data_strict(data, train_ratio=0.5)\n",
    "# historic.sort_values([dataset_manager.timestamp_col], ascending=True, kind='mergesort', inplace=True)\n",
    "# current.sort_values([dataset_manager.timestamp_col], ascending=True, kind='mergesort', inplace=True)\n",
    "# current = current.head(200)\n",
    "\n",
    "# features_used = historic.columns.difference(\n",
    "#     [dataset_manager.label_col, dataset_manager.timestamp_col, dataset_manager.case_id_col, 'event_nr', 'case_length', 'finished'], \n",
    "#     sort=False\n",
    "# )\n",
    "# end = time.time()\n",
    "# print(f\"Data preprocessing completed in {end - start:.2f} seconds.\")\n",
    "\n",
    "# # Precompute and pass only picklable objects\n",
    "# start = time.time()\n",
    "# preprocessed_data = {\n",
    "#     'historic': historic.to_dict(),  # Convert DataFrame to dictionary\n",
    "#     'current': current.to_dict(),\n",
    "#     'features_used': list(features_used)  # Convert Index to list\n",
    "# }\n",
    "\n",
    "# dataset_manager_data = {\n",
    "#     'case_id_col': dataset_manager.case_id_col,\n",
    "#     'timestamp_col': dataset_manager.timestamp_col,\n",
    "#     'label_col': dataset_manager.label_col,\n",
    "#     'activity_col': dataset_manager.activity_col,\n",
    "#     'static_cat_cols': dataset_manager.static_cat_cols\n",
    "# }\n",
    "# end = time.time()\n",
    "# print(\"Preprocessed data ready for parallel processing in {:.2f} seconds.\".format(end - start))\n",
    "\n",
    "# def process_combination(params, preprocessed_data, dataset_manager_data):\n",
    "#     start = time.time()\n",
    "#     results = []\n",
    "#     AUCs = []\n",
    "#     print(f\"Testing combination: {params}\")\n",
    "    \n",
    "#     # Extract preprocessed data\n",
    "#     historic = pd.DataFrame(preprocessed_data['historic'])\n",
    "#     current = pd.DataFrame(preprocessed_data['current'])\n",
    "#     features_used = preprocessed_data['features_used']\n",
    "    \n",
    "#     # Configure preprocessor\n",
    "#     if params['encoding_method'] == 'quantile':\n",
    "#         preprocessor = make_column_transformer(\n",
    "#             (StandardScaler(), make_column_selector(dtype_include=['int64', 'float64'])),\n",
    "#             (ce.quantile_encoder.SummaryEncoder(), make_column_selector(dtype_include=['object', 'category'])),\n",
    "#             remainder='drop'\n",
    "#         )\n",
    "#     elif params['encoding_method'] == 'onehot':\n",
    "#         preprocessor = make_column_transformer(\n",
    "#             (OneHotEncoder(handle_unknown=\"ignore\"), make_column_selector(dtype_include=['object', 'category'])),\n",
    "#             remainder='passthrough'\n",
    "#         )\n",
    "    \n",
    "#     # Transform data once\n",
    "#     historic_transformed = preprocessor.fit_transform(historic[features_used], historic[dataset_manager_data['label_col']])\n",
    "#     current_transformed = preprocessor.transform(current[features_used])\n",
    "    \n",
    "#     # Train nearest neighbors model\n",
    "#     nn_model = NearestNeighbors(n_neighbors=params['num_nearest_neighbors'], metric=params['distance_metric'])\n",
    "#     nn_model.fit(historic_transformed)\n",
    "\n",
    "#     batch_size = 100  # Define batch size for processing\n",
    "    \n",
    "#     # Process batches\n",
    "#     for start in range(0, len(current), batch_size):  # Batch size = 100\n",
    "#         batch = current.iloc[start:start+batch_size]\n",
    "#         distances, indices = nn_model.kneighbors(current_transformed[start:start+batch_size])\n",
    "#         nearest_neighbors = pd.concat([historic.iloc[indices[i]] for i in range(len(batch))])\n",
    "        \n",
    "#         target = nearest_neighbors[dataset_manager_data['label_col']].values\n",
    "#         target_test = batch[dataset_manager_data['label_col']]\n",
    "        \n",
    "#         X_train, X_cal, y_train, y_cal = train_test_split(nearest_neighbors, target, test_size=0.3, random_state=42)\n",
    "        \n",
    "#         # Train model\n",
    "#         if params['model'] == 'Catboost':\n",
    "#             model = CatBoostClassifier(iterations=100, loss_function='Logloss', eval_metric='AUC', verbose=0, cat_features=[dataset_manager_data['activity_col']]+dataset_manager_data['static_cat_cols'])\n",
    "#             model.fit(X_train[features_used], y_train, cat_features=[dataset_manager_data['activity_col']]+dataset_manager_data['static_cat_cols'])\n",
    "#         elif params['model'] == 'LogisticRegression':\n",
    "#             model = make_pipeline(preprocessor, LogisticRegression())\n",
    "#             model.fit(X_train[features_used], y_train)\n",
    "#         elif params['model'] == 'HMM':\n",
    "#             model = hmm.GaussianHMM(n_components=7)\n",
    "#             model.fit(nearest_neighbors[features_used])\n",
    "        \n",
    "#         # Calibrate and predict\n",
    "#         # calibrated_model = CalibratedClassifierCV(model, method='sigmoid', cv='prefit')\n",
    "#         calibrated_model = CalibratedClassifierCV(FrozenEstimator(model), method='sigmoid')\n",
    "#         calibrated_model.fit(X_cal[features_used], y_cal)\n",
    "#         probs = calibrated_model.predict_proba(batch[features_used])[:, 1]\n",
    "#         y_true = batch[dataset_manager_data['label_col']].values\n",
    "        \n",
    "#         accuracy = accuracy_score(y_true, calibrated_model.predict(batch[features_used]))\n",
    "#         f1 = f1_score(y_true, calibrated_model.predict(batch[features_used]), average='weighted')\n",
    "#         precision = precision_score(y_true, calibrated_model.predict(batch[features_used]), average='weighted', zero_division=0)\n",
    "#         recall = recall_score(y_true, calibrated_model.predict(batch[features_used]), average='weighted', zero_division=0)\n",
    "        \n",
    "#         if len(set(y_true)) > 1:\n",
    "#             auc = roc_auc_score(y_true, probs)\n",
    "#             AUCs.append(auc)\n",
    "#         else:\n",
    "#             auc = None\n",
    "        \n",
    "#         results.append({\n",
    "#             'params': params,\n",
    "#             'auc': sum(AUCs) / len(AUCs) if AUCs else None,\n",
    "#             'accuracy' : accuracy,\n",
    "#             'f1_score': f1,\n",
    "#             'precision': precision,\n",
    "#             'recall': recall,\n",
    "#         })\n",
    "#     end = time.time()\n",
    "#     print(f\"Combination processed in {end - start:.2f} seconds.\")\n",
    "    \n",
    "#     return results\n",
    "\n",
    "\n",
    "# # Parallelize grid search\n",
    "# all_results = Parallel(n_jobs=-1)(\n",
    "#     delayed(process_combination)(params, preprocessed_data, dataset_manager_data) for params in ParameterGrid(param_grid)\n",
    "# )\n",
    "\n",
    "# # Flatten results and save\n",
    "# results = [item for sublist in all_results for item in sublist]\n",
    "# results_df = pd.DataFrame(results)\n",
    "# results_df.to_csv('grid_search_results.csv', index=False)\n",
    "\n",
    "# # Print the best combination\n",
    "# best_result = results_df.loc[results_df['f1_score'].idxmax()]\n",
    "# print(f\"Best combination: {best_result}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f91a2e7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessed data ready for parallel processing in 0.00 seconds.\n"
     ]
    }
   ],
   "source": [
    "# Precompute and pass only picklable objects\n",
    "start = time.time()\n",
    "preprocessed_data = {\n",
    "    'historic': historic,  # Convert DataFrame to dictionary\n",
    "    'current': current,\n",
    "    'features_used': list(features_used)  # Convert Index to list\n",
    "}\n",
    "\n",
    "dataset_manager_data = {\n",
    "    'case_id_col': dataset_manager.case_id_col,\n",
    "    'timestamp_col': dataset_manager.timestamp_col,\n",
    "    'label_col': dataset_manager.label_col,\n",
    "    'activity_col': dataset_manager.activity_col,\n",
    "    'static_cat_cols': dataset_manager.static_cat_cols\n",
    "}\n",
    "end = time.time()\n",
    "print(\"Preprocessed data ready for parallel processing in {:.2f} seconds.\".format(end - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "362aba3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing combination: {'num_nearest_neighbors': 100, 'distance_metric': 'euclidean', 'encoding_method': 'catboost', 'model': 'Catboost', 'batch_size': 100}\n",
      "Combination processed in 0.09 minutes.\n",
      "Testing combination: {'num_nearest_neighbors': 100, 'distance_metric': 'euclidean', 'encoding_method': 'count', 'model': 'Catboost', 'batch_size': 100}\n",
      "Combination processed in 0.09 minutes.\n",
      "Testing combination: {'num_nearest_neighbors': 100, 'distance_metric': 'euclidean', 'encoding_method': 'onehot', 'model': 'Catboost', 'batch_size': 100}\n",
      "Combination processed in 0.09 minutes.\n",
      "Testing combination: {'num_nearest_neighbors': 100, 'distance_metric': 'euclidean', 'encoding_method': 'target', 'model': 'Catboost', 'batch_size': 100}\n",
      "Combination processed in 0.09 minutes.\n",
      "Testing combination: {'num_nearest_neighbors': 100, 'distance_metric': 'euclidean', 'encoding_method': 'woe', 'model': 'Catboost', 'batch_size': 100}\n",
      "Combination processed in 0.09 minutes.\n",
      "Testing combination: {'num_nearest_neighbors': 100, 'distance_metric': 'euclidean', 'encoding_method': 'rank', 'model': 'Catboost', 'batch_size': 100}\n",
      "Combination processed in 0.11 minutes.\n"
     ]
    }
   ],
   "source": [
    "def process_combination(params, preprocessed_data, dataset_manager_data):\n",
    "    start_time = time.time()\n",
    "    results = []\n",
    "    AUCs = []\n",
    "    results_df = pd.DataFrame()\n",
    "    results_dicts = []\n",
    "    print(f\"Testing combination: {params}\")\n",
    "    \n",
    "    # Extract preprocessed data\n",
    "    historic = pd.DataFrame(preprocessed_data['historic'])\n",
    "    current = pd.DataFrame(preprocessed_data['current'])\n",
    "    features_used = preprocessed_data['features_used']\n",
    "    \n",
    "    # Configure preprocessor\n",
    "    if params['encoding_method'] == 'quantile':\n",
    "        preprocessor = make_column_transformer(\n",
    "            (StandardScaler(), make_column_selector(dtype_include=['int64', 'float64'])),\n",
    "            (ce.quantile_encoder.SummaryEncoder(), make_column_selector(dtype_include=['object', 'category'])),\n",
    "            remainder='drop'\n",
    "        )\n",
    "    elif params['encoding_method'] == 'onehot':\n",
    "        preprocessor = make_column_transformer(\n",
    "            (StandardScaler(), make_column_selector(dtype_include=['int64', 'float64'])),\n",
    "            (ce.OneHotEncoder(), make_column_selector(dtype_include=['object', 'category'])),\n",
    "            remainder='drop'\n",
    "        )\n",
    "    elif params['encoding_method'] == 'catboost':\n",
    "        preprocessor = make_column_transformer(\n",
    "            (StandardScaler(), make_column_selector(dtype_include=['int64', 'float64'])),\n",
    "            (ce.CatBoostEncoder(), make_column_selector(dtype_include=['object', 'category'])),\n",
    "            remainder='drop'\n",
    "        )\n",
    "    elif params['encoding_method'] == 'count':\n",
    "        preprocessor = make_column_transformer(\n",
    "            (StandardScaler(), make_column_selector(dtype_include=['int64', 'float64'])),\n",
    "            (ce.CountEncoder(normalize=True), make_column_selector(dtype_include=['object', 'category'])),\n",
    "            remainder='drop'\n",
    "        )\n",
    "    elif params['encoding_method'] == 'target':\n",
    "        preprocessor = make_column_transformer(\n",
    "            (StandardScaler(), make_column_selector(dtype_include=['int64', 'float64'])),\n",
    "            (ce.TargetEncoder(), make_column_selector(dtype_include=['object', 'category'])),\n",
    "            remainder='drop'\n",
    "        )\n",
    "    elif params['encoding_method'] == 'woe':\n",
    "        preprocessor = make_column_transformer(\n",
    "            (StandardScaler(), make_column_selector(dtype_include=['int64', 'float64'])),\n",
    "            (ce.WOEEncoder(), make_column_selector(dtype_include=['object', 'category'])),\n",
    "            remainder='drop'\n",
    "        )\n",
    "    elif params['encoding_method'] == 'rank':\n",
    "        preprocessor = make_column_transformer(\n",
    "            (StandardScaler(), make_column_selector(dtype_include=['int64', 'float64'])),\n",
    "            (ce.RankHotEncoder(), make_column_selector(dtype_include=['object', 'category'])),\n",
    "            remainder='drop'\n",
    "        )\n",
    "    \n",
    "    # Transform data once\n",
    "    historic_transformed = preprocessor.fit_transform(historic[features_used], historic[dataset_manager_data['label_col']])\n",
    "    current_transformed = preprocessor.transform(current[features_used])\n",
    "    \n",
    "    # Train nearest neighbors model\n",
    "    nn_model = NearestNeighbors(n_neighbors=params['num_nearest_neighbors'], metric=params['distance_metric'])\n",
    "    nn_model.fit(historic_transformed)\n",
    "\n",
    "    batch_size = params['batch_size']  # Define batch size for processing\n",
    "    \n",
    "    # Process batches\n",
    "    for start in range(0, len(current), batch_size):  # Batch size = 100\n",
    "        batch = current.iloc[start:start+batch_size]\n",
    "        batch_transformed = preprocessor.transform(batch[features_used])\n",
    "        distances, indices = nn_model.kneighbors(batch_transformed)\n",
    "        nearest_neighbors = pd.concat([historic.iloc[indices[i]] for i in range(len(batch))])\n",
    "        \n",
    "        target = nearest_neighbors[dataset_manager_data['label_col']].values\n",
    "        target_test = batch[dataset_manager_data['label_col']]\n",
    "        \n",
    "        X_train, X_cal, y_train, y_cal = train_test_split(nearest_neighbors, target, test_size=0.3, random_state=42)\n",
    "        \n",
    "        # Train model\n",
    "        if params['model'] == 'Catboost':\n",
    "            model = CatBoostClassifier(iterations=100, loss_function='Logloss', eval_metric='AUC', verbose=0, cat_features=[dataset_manager_data['activity_col']]+dataset_manager_data['static_cat_cols'],random_seed=42)\n",
    "            model.fit(X_train[features_used], y_train, cat_features=[dataset_manager_data['activity_col']]+dataset_manager_data['static_cat_cols'])\n",
    "        elif params['model'] == 'LogisticRegression':\n",
    "            model = make_pipeline(preprocessor, LogisticRegression())\n",
    "            model.fit(X_train[features_used], y_train)\n",
    "        elif params['model'] == 'HMM':\n",
    "            model = hmm.GaussianHMM(n_components=7)\n",
    "            model.fit(nearest_neighbors[features_used])\n",
    "        \n",
    "        # Calibrate and predict\n",
    "        # calibrated_model = CalibratedClassifierCV(model, method='sigmoid', cv='prefit')\n",
    "        calibrated_model = CalibratedClassifierCV(FrozenEstimator(model), method='sigmoid')\n",
    "        calibrated_model.fit(X_cal[features_used], y_cal)\n",
    "\n",
    "        # Predict probabilities and labels\n",
    "        probs = calibrated_model.predict_proba(batch[features_used])[:, 1]\n",
    "        preds = calibrated_model.predict(batch[features_used])\n",
    "        y_true = batch[dataset_manager_data['label_col']].values\n",
    "        \n",
    "        if len(set(y_true)) > 1:\n",
    "            auc = roc_auc_score(y_true, probs)\n",
    "            AUCs.append(auc)\n",
    "        else:\n",
    "            auc = None\n",
    "\n",
    "        # Check if array has null values\n",
    "        has_null = pd.Series(preds.flatten(), index=batch.index).isna().any()\n",
    "\n",
    "        if has_null:\n",
    "            print(\"Preds has null values\")\n",
    "\n",
    "        batch.loc[:, 'predicted_value'] = pd.Series(preds.flatten(), index=batch.index)\n",
    "        batch.loc[:, 'proba_of_regular'] = pd.Series(probs.flatten(), index=batch.index)\n",
    "        is_null = batch['predicted_value'].isna().any()\n",
    "        if is_null:\n",
    "            print(\"Batch has null values\")\n",
    "\n",
    "        results.append(batch)\n",
    "\n",
    "        # Add the current row with its prediction to the historic data\n",
    "        finished_case_ids = batch[batch['finished'] == 1][dataset_manager_data['case_id_col']].unique()\n",
    "        finished_cases = current[current[dataset_manager_data['case_id_col']].isin(finished_case_ids)]\n",
    "        historic = pd.concat([historic, finished_cases], ignore_index=True)\n",
    "        historic.sort_values([dataset_manager_data['case_id_col'], dataset_manager_data['timestamp_col']], ascending=True, kind='mergesort', inplace=True)\n",
    "        historic_transformed = preprocessor.fit_transform(historic[features_used], historic[dataset_manager.label_col])\n",
    "        nn_model.fit(historic_transformed) # Refit the model with the updated historic data\n",
    "        \n",
    "    results_df = pd.concat(results)\n",
    "\n",
    "    # Calculate metrics\n",
    "    true_values = results_df[dataset_manager.label_col]\n",
    "    predicted_values = results_df['predicted_value']\n",
    "    proba_values = results_df['proba_of_regular']\n",
    "\n",
    "    accuracy = accuracy_score(true_values, predicted_values)\n",
    "    f1 = f1_score(true_values, predicted_values, average='weighted')\n",
    "    precision = precision_score(true_values, predicted_values, average='weighted', zero_division=0)\n",
    "    recall = recall_score(true_values, predicted_values, average='weighted', zero_division=0)\n",
    "\n",
    "    results_dicts.append({\n",
    "        'params': params,\n",
    "        'auc': sum(AUCs) / len(AUCs) if AUCs else None,\n",
    "        'accuracy' : accuracy,\n",
    "        'f1_score': f1,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "    })\n",
    "    end_time = time.time()\n",
    "    print(f\"Combination processed in {(end_time - start_time) / 60:.2f} minutes.\")\n",
    "    \n",
    "    return results_dicts, results_df\n",
    "\n",
    "\n",
    "# Sequentially process each combination\n",
    "all_results = []\n",
    "for params in param_dicts:\n",
    "    results_dict, results_df = process_combination(params, preprocessed_data, dataset_manager_data)\n",
    "    all_results.extend(results_dict)  # Append results for this combination\n",
    "\n",
    "    output_dir = Path(f\"results_HP_tuning/{dataset_name}\")\n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "    results_df.to_csv(f'{output_dir}/{params}.csv', index=False)\n",
    "\n",
    "# Flatten results and save\n",
    "results_all_df = pd.DataFrame(all_results)\n",
    "results_all_df.to_csv('grid_search_results.csv', index=False)\n",
    "\n",
    "# Print the best combination\n",
    "best_result = results_all_df.loc[results_all_df['f1_score'].idxmax()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7851084",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(f\"Best combination: {best_result}\")\n",
    "print(best_result['params'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26ef6e2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "########## orginal implementation of JITL ############\n",
    "\n",
    "\n",
    "# # Define the values to experiment with\n",
    "# results = []\n",
    "# trainingtimes = []\n",
    "# AUCs = []\n",
    "# method = 'Catboost'\n",
    "# num_nearest_neighbors_values = [300]\n",
    "# distance_metrics = ['euclidean']\n",
    "# results_df = pd.DataFrame()\n",
    "\n",
    "# # Loop through the values and create processors\n",
    "# for num_nearest_neighbors in num_nearest_neighbors_values:\n",
    "#     for distance_metric in distance_metrics:\n",
    "#         print(f\"Experimenting with num_nearest_neighbors={num_nearest_neighbors} and distance_metric={distance_metric}\")\n",
    "        \n",
    "#         # Create the processor with the current values\n",
    "#         processor = processor_factory.get_processor(dataset_name, use_encoding=False, use_bucketing=False, num_nearest_neighbors=num_nearest_neighbors, distance_metric=distance_metric)\n",
    "\n",
    "#         data = merged_df.sort_values([dataset_manager.case_id_col, dataset_manager.timestamp_col], ascending=True, kind='mergesort')\n",
    "#         historic, current = processor.split_data_strict(data, train_ratio=0.5)\n",
    "#         historic.sort_values([dataset_manager.timestamp_col], ascending=True, kind='mergesort', inplace=True)\n",
    "#         current.sort_values([dataset_manager.timestamp_col], ascending=True, kind='mergesort', inplace=True)\n",
    "#         current = current.head(200)\n",
    "\n",
    "\n",
    "#         # Exclude specific columns from historic\n",
    "#         features_used = historic.columns.difference([dataset_manager.label_col, dataset_manager.timestamp_col, dataset_manager.case_id_col, 'event_nr', 'case_length', 'finished'], sort=False)\n",
    "   \n",
    "#         batch_size = 100\n",
    "\n",
    "#         initial_start_time = time.time()\n",
    "#         # Define the preprocessor\n",
    "#         preprocessor = make_column_transformer(\n",
    "#         (StandardScaler(), make_column_selector(dtype_include=['int64', 'float64'])),  # Scale continuous variables\n",
    "#         (ce.quantile_encoder.SummaryEncoder(), make_column_selector(dtype_include=['object', 'category'])),  # Encode categorical variables\n",
    "#         remainder='drop')  # Drop other columns\n",
    "#         nn_model = NearestNeighbors(n_neighbors=num_nearest_neighbors, metric=distance_metric)\n",
    "#         historic_transformed = preprocessor.fit_transform(historic[features_used], historic[dataset_manager.label_col])\n",
    "#         nn_model.fit(historic_transformed)\n",
    "#         initial_end_time = time.time()\n",
    "\n",
    "#         initial_training_time = initial_end_time - initial_start_time\n",
    "#         print(\"Initial Model Training time: \", initial_training_time, \" seconds\")\n",
    "\n",
    "#         for start in range(0, len(current), batch_size):\n",
    "#             end = start + batch_size\n",
    "#             batch = current.iloc[start:end]\n",
    "#             # Find the n nearest neighbors for the selected row\n",
    "#             distances, indices = nn_model.kneighbors(preprocessor.transform(batch[features_used]))\n",
    "#             nearest_neighbors = pd.concat([historic.iloc[indices[i]] for i in range(len(batch))])\n",
    "#             print(nearest_neighbors.shape)\n",
    "\n",
    "#             target = nearest_neighbors[dataset_manager.label_col].values\n",
    "#             target_test = batch[dataset_manager.label_col]\n",
    "\n",
    "#             # Split into training and calibration sets\n",
    "#             X_train, X_cal, y_train, y_cal = train_test_split(nearest_neighbors, target, test_size=0.3, random_state=42)\n",
    "\n",
    "#             if target_test is None:\n",
    "#                 continue\n",
    "            \n",
    "\n",
    "#             start_time = time.time()\n",
    "#             if method == 'Catboost':\n",
    "#                 model = CatBoostClassifier(iterations=100, loss_function='Logloss', eval_metric='AUC', verbose=0, cat_features=[dataset_manager.activity_col]+dataset_manager.static_cat_cols)\n",
    "#                 print('Now training')\n",
    "#                 model.fit(X_train[features_used], y_train, cat_features=[dataset_manager.activity_col]+dataset_manager.static_cat_cols)\n",
    "\n",
    "#             if method == 'HMM':\n",
    "#                 # Create an instance of the HMM model\n",
    "#                 model = hmm.GaussianHMM(n_components=7)  # Specify the number of hidden states\n",
    "#                 model.fit(nearest_neighbors[features_used])\n",
    "\n",
    "#             if method == 'LogisticRegression':\n",
    "\n",
    "#                 preprocessor = make_column_transformer(\n",
    "#                     (OneHotEncoder(handle_unknown=\"ignore\"), make_column_selector(dtype_include='object')),\n",
    "#                     remainder='passthrough'\n",
    "#                 )\n",
    "#                 model = make_pipeline(\n",
    "#                     preprocessor,\n",
    "#                     LogisticRegression()\n",
    "#                 )\n",
    "#                 model.fit(X_train[features_used], y_train)\n",
    "#             end_time = time.time()\n",
    "\n",
    "#             training_time = (end_time - start_time) / 60\n",
    "#             print(\"Training time: \", training_time, \" minutes\")\n",
    "#             trainingtimes.append(training_time)\n",
    "\n",
    "#             # Step 2: Wrap with calibration on held-out calibration data\n",
    "#             calibrated_model = CalibratedClassifierCV(model, method='sigmoid', cv='prefit')\n",
    "#             calibrated_model.fit(X_cal[features_used], y_cal)\n",
    "\n",
    "#             # Make predictions on the testing data\n",
    "#             print('Now predicting')\n",
    "#             preds = model.predict(batch[features_used])\n",
    "            \n",
    "#             # Step 3: Predict calibrated probabilities for the query instance\n",
    "#             probs = calibrated_model.predict_proba(batch[features_used])[:, 1]\n",
    "#             y_true = batch[dataset_manager.label_col].values\n",
    "#             # Check if y_true contains at least two classes\n",
    "#             if len(set(y_true)) > 1:\n",
    "#                 auc = roc_auc_score(y_true, probs)\n",
    "#                 AUCs.append(auc)\n",
    "#             else:\n",
    "#                 print(\"Warning: Only one class present in y_true. Skipping ROC AUC calculation.\")\n",
    "#                 auc = None\n",
    "\n",
    "#             cal_probs = calibrated_model.predict_proba(X_cal[features_used])[:, 1]\n",
    "#             cal_true = X_cal[dataset_manager.label_col]\n",
    "#             best_threshold, best_accuracy = find_threshold(cal_probs, cal_true)\n",
    "#             preds_thr = probs > np.median(best_threshold)\n",
    "\n",
    "#             # Check if array has null values\n",
    "#             has_null = pd.Series(preds.flatten(), index=batch.index).isna().any()\n",
    "\n",
    "#             if has_null:\n",
    "#                 print(\"Preds has null values\")\n",
    "\n",
    "#             batch.loc[:, 'predicted_value'] = pd.Series(preds.flatten(), index=batch.index)\n",
    "#             batch.loc[:, 'proba_of_regular'] = pd.Series(probs.flatten(), index=batch.index)\n",
    "#             batch.loc[:, 'predicted_thr'] = pd.Series(preds_thr.flatten(), index=batch.index)\n",
    "#             batch.loc[:, 'predicted_thr'] = batch['predicted_thr'].map({False: 'deviant', True: 'regular'})\n",
    "#             is_null = batch['predicted_value'].isna().any()\n",
    "#             if is_null:\n",
    "#                 print(\"Batch has null values\")\n",
    "\n",
    "#             results.append(batch)\n",
    "\n",
    "#             # Add the current row with its prediction to the historic data\n",
    "#             finished_case_ids = batch[batch['finished'] == 1][dataset_manager.case_id_col].unique()\n",
    "#             finished_cases = current[current[dataset_manager.case_id_col].isin(finished_case_ids)]\n",
    "#             historic = pd.concat([historic, finished_cases], ignore_index=True)\n",
    "#             historic.sort_values([dataset_manager.case_id_col, dataset_manager.timestamp_col], ascending=True, kind='mergesort', inplace=True)\n",
    "#             historic_transformed = preprocessor.fit_transform(historic[features_used], historic[dataset_manager.label_col])\n",
    "#             nn_model.fit(historic_transformed) # Refit the model with the updated historic data\n",
    "\n",
    "#         results_df = pd.concat(results)\n",
    "\n",
    "#         # Calculate metrics\n",
    "#         true_values = results_df[dataset_manager.label_col]\n",
    "#         predicted_values = results_df['predicted_value']\n",
    "#         proba_values = results_df['proba_of_regular']\n",
    "\n",
    "#         accuracy = np.mean(true_values == predicted_values)\n",
    "#         accuracy_thr = np.mean(true_values == results_df['predicted_thr'])\n",
    "\n",
    "#         # Print metrics\n",
    "#         print(f\"Accuracy: {accuracy}\")\n",
    "#         print(f\"Accuracy (Threshold): {accuracy_thr}\")\n",
    "#         print(f\"f1_score: {f1_score(true_values, predicted_values, average='weighted')}\")\n",
    "#         print(f\"Training Time: {sum(trainingtimes)/len(trainingtimes)}\")\n",
    "#         print(f\"AUC: {sum(AUCs)/len(AUCs)}\")\n",
    "\n",
    "#         output_dir = Path(f\"results_HP_tuning/{dataset_name}\")\n",
    "#         output_dir.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "#         results_df.to_csv(f'{output_dir}/{method}_{num_nearest_neighbors}_{distance_metric}.csv', index=False)\n",
    "#         print('***********************************')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5d0f659",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate metrics and save for best combo\n",
    "best = pd.read_csv(f\"{output_dir}/{best_result[\"params\"]}.csv\")\n",
    "proposed_metrics = calculate_metrics(best, dataset_manager)\n",
    "save_results_to_csv(f'{dataset_name}', proposed_metrics, baseline_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3ce5f94",
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = pd.read_csv(f'{output_dir}/baseline_{method}.csv')\n",
    "tmp.sort_values([dataset_manager.timestamp_col], ascending=True, kind='mergesort', inplace=True)\n",
    "\n",
    "tmp2 = pd.read_csv(f\"{output_dir}/{best_result[\"params\"]}.csv\")\n",
    "tmp2.sort_values([dataset_manager.timestamp_col], ascending=True, kind='mergesort', inplace=True)\n",
    "\n",
    "# Calculate moving average accuracy for tmp and tmp2\n",
    "mae_df_tmp = calculate_moving_avg_acc_fast(tmp, dataset_manager.label_col, 'predicted_value', dataset_manager)\n",
    "mae_df_tmp2 = calculate_moving_avg_acc_fast(tmp2, dataset_manager.label_col, 'predicted_value', dataset_manager)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d090dc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the moving average accuracy for both DataFrames\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(mae_df_tmp['num_rows'], mae_df_tmp['moving_avg_mae'], label='baseline', color='blue')\n",
    "plt.plot(mae_df_tmp2['num_rows'], mae_df_tmp2['moving_avg_mae'], label='JIT-Cat', color='red')\n",
    "# plt.plot(mae_df_tmp3['num_rows'], mae_df_tmp3['moving_avg_mae'], label='euclidean', color='green')\n",
    "# plt.plot(mae_df_tmp4['num_rows'], mae_df_tmp4['moving_avg_mae'], label='DTW', color='black')\n",
    "plt.xlabel('Number of Observed Events over Time')\n",
    "plt.ylabel('Moving Average Accuracy')\n",
    "plt.title(f'{dataset_name} Moving Average Accuracy')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "# plt.show()\n",
    "plt.savefig(f\"results_HP_tuning/{dataset_name}/moving_avg_accuracy_score.png\", dpi=600, bbox_inches='tight')\n",
    "plt.savefig(f\"results_HP_tuning/{dataset_name}/moving_avg_accuracy_score.pdf\", dpi=600, bbox_inches='tight', format=\"pdf\")  # Save as PDF\n",
    "\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feee4a2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the moving average F1 score for both DataFrames\n",
    "# plt.figure(figsize=(10, 6))\n",
    "# plt.plot(mae_df_tmp['num_rows'], mae_df_tmp['moving_avg_f1'], label='baseline', color='blue')\n",
    "# plt.plot(mae_df_tmp2['num_rows'], mae_df_tmp2['moving_avg_f1'], label='JIT-Cat', color='red')\n",
    "# # plt.plot(mae_df_tmp3['num_rows'], mae_df_tmp3['moving_avg_mae'], label='euclidean', color='green')\n",
    "# # plt.plot(mae_df_tmp4['num_rows'], mae_df_tmp4['moving_avg_mae'], label='DTW', color='black')\n",
    "# plt.xlabel('Number of Observed Events over Time')\n",
    "# plt.ylabel('Moving Average F1 Score')\n",
    "# plt.title(f'{dataset_name} Moving Average F1 Score')\n",
    "# plt.legend()\n",
    "# plt.grid(True)\n",
    "# # plt.show()\n",
    "# plt.savefig(f\"results/{dataset_name}/moving_avg_f1_score.png\", dpi=600, bbox_inches='tight')\n",
    "# plt.savefig(f\"results/{dataset_name}/moving_avg_F1_score.pdf\", dpi=600, bbox_inches='tight', format=\"pdf\")  # Save as PDF\n",
    "# plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d19d0cb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12a459ba",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
