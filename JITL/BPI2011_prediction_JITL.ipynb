{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5a683265-5556-4ea9-b537-477150acee41",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "\n",
    "from pathlib import Path\n",
    "from torch import cdist\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fc01ccd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from catboost import CatBoostClassifier\n",
    "from hmmlearn import hmm\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score, mean_absolute_percentage_error\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "import math\n",
    "# from statsmodels.tsa.arima.model import ARIMA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c4292a19",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "38ab6a18",
   "metadata": {},
   "outputs": [],
   "source": [
    "ROOT_DIR = Path().cwd()\n",
    "while not ROOT_DIR.joinpath(\"data\").exists():\n",
    "    ROOT_DIR = ROOT_DIR.parent\n",
    "os.chdir(ROOT_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "290bd199",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AMBR\n",
      "CSL312\n",
      "Astrazeneca\n",
      "bpic2011_f1\n",
      "bpic2011_f2\n",
      "bpic2011_f3\n",
      "bpic2011_f4\n"
     ]
    }
   ],
   "source": [
    "from processors import processor_factory\n",
    "from data.DatasetManager import DatasetManager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fd096c8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "case_id_col = 'Case ID'\n",
    "timestamp_col = 'time:timestamp'\n",
    "target_col = \"Activity code\"\n",
    "dataset_name = 'bpic2011_f1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3958550e",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_to_keep = [case_id_col, timestamp_col, target_col, 'BR Temp (°C)', 'pH (Internal)', 'Viability (%)', 'VCD (10^6 cells/mL)', \n",
    "                'Glutamine (mmol/L)', 'Glutamate (mmol/L)', 'Glucose (g/L)','Lactate (g/L)', 'NH4+ (mmol/L)', \n",
    "                'Na+ (mmol/L)', 'K+ (mmol/L)', 'Ca2+ (g/L)']\n",
    "\n",
    "features = ['BR Temp (°C)', 'pH (Internal)', 'Viability (%)', 'VCD (10^6 cells/mL)', \n",
    "                'Glutamine (g/L)', 'Glutamate (g/L)', 'Glucose (g/L)','Lactate (g/L)', 'NH4+ (g/L)', \n",
    "                'Na+ (g/L)', 'K+ (g/L)', 'Ca2+ (g/L)']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7e0399c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/processed_benchmark_event_logs/BPIC11_f1.csv', sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9fc93362",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[timestamp_col] = pd.to_datetime(df[timestamp_col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9634f96e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sort_values([case_id_col, timestamp_col], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9d06bbee",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_manager = DatasetManager(dataset_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "41dd5fe5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1140"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[case_id_col].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5e3da3fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Diagnosis', 'Treatment code', 'Diagnosis code', 'Specialism code',\n",
       "       'Diagnosis Treatment Combination ID', 'Age', 'Case ID', 'label',\n",
       "       'Activity code', 'Producer code', 'Section', 'Specialism code.1',\n",
       "       'group', 'Number of executions', 'time:timestamp', 'timesincemidnight',\n",
       "       'month', 'weekday', 'hour', 'timesincelastevent', 'timesincecasestart',\n",
       "       'event_nr', 'open_cases'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "639346c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Case ID\n",
       "0      2005-01-02 23:00:00\n",
       "1      2005-01-02 23:00:00\n",
       "2      2005-01-02 23:00:00\n",
       "3      2005-01-02 23:00:00\n",
       "4      2005-01-02 23:00:00\n",
       "               ...        \n",
       "1138   2008-03-11 23:00:00\n",
       "1139   2008-03-11 23:00:00\n",
       "1140   2008-03-16 23:00:00\n",
       "1141   2008-03-17 23:00:00\n",
       "1142   2008-03-19 23:00:00\n",
       "Name: time:timestamp, Length: 1140, dtype: datetime64[ns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "earliest_timestamps = df.groupby(case_id_col)[timestamp_col].min()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "99ed3c18",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_first_n_cases(df, n):\n",
    "    earliest_timestamps = df.groupby(case_id_col)[timestamp_col].min()\n",
    "    sorted_cases = earliest_timestamps.sort_values().index[:n]\n",
    "    return df[df[case_id_col].isin(sorted_cases)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "222f4d89",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df = get_first_n_cases(df, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f4db4dfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train, df_test = dataset_manager.split_data_strict(filtered_df, 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "265a9099",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "method = 'Catboost'\n",
    "config = 'no_encoding_no_bucketing'\n",
    "# config = 'no_encoding_bucketing'\n",
    "# config = 'encoding_no_bucketing'\n",
    "# config = 'encoding_bucketing'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b173b67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from data.dataset_confs import dataset_configs\n",
    "\n",
    "# config = dataset_configs['CSL_5L']\n",
    "use_encoding = False\n",
    "use_bucketing = False\n",
    "num_nearest_neighbors = 100\n",
    "distance_metric = 'euclidean'\n",
    "\n",
    "# processor = CSL_5LProcessor(\"5L\", use_encoding, use_bucketing, num_nearest_neighbors, distance_metric)\n",
    "processor = processor_factory.get_processor(dataset_name, use_encoding, use_bucketing, num_nearest_neighbors, distance_metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf37f339",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26ef6e2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the values to experiment with\n",
    "results = []\n",
    "num_nearest_neighbors_values = [200]\n",
    "distance_metrics = ['euclidean']\n",
    "\n",
    "# Loop through the values and create processors\n",
    "for num_nearest_neighbors in num_nearest_neighbors_values:\n",
    "    for distance_metric in distance_metrics:\n",
    "        print(f\"Experimenting with num_nearest_neighbors={num_nearest_neighbors} and distance_metric={distance_metric}\")\n",
    "        \n",
    "        # Create the processor with the current values\n",
    "        processor = processor_factory.get_processor(dataset_name, use_encoding=False, use_bucketing=False, num_nearest_neighbors=num_nearest_neighbors, distance_metric=distance_metric)\n",
    "        \n",
    "    \n",
    "\n",
    "        data = df.sort_values([case_id_col, timestamp_col], ascending=True, kind='mergesort')\n",
    "        data['Target'] = data.groupby(case_id_col)[target_col].shift(-1)\n",
    "        # data['Target_orig'] = data.groupby(case_id_col)['Titer (g/L) original'].shift(-1)\n",
    "        data['Target'] = data.groupby(case_id_col)['Target'].ffill().bfill()\n",
    "        # data['Target_orig'] = data.groupby(case_id_col)['Target_orig'].ffill()\n",
    "\n",
    "        historic, current = processor.split_data_strict(data, train_ratio=0.5)\n",
    "        historic.sort_values([case_id_col, timestamp_col], ascending=True, kind='mergesort', inplace=True)\n",
    "        current.sort_values([timestamp_col], ascending=True, kind='mergesort', inplace=True)\n",
    "\n",
    "        if config == 'no_encoding_bucketing' or config == 'encoding_bucketing':\n",
    "            features_used = processor.static_num_cols + processor.dynamic_num_cols + processor.static_cat_cols + processor.dynamic_cat_cols + ['Cluster']\n",
    "        else:\n",
    "            features_used = processor.static_num_cols + processor.dynamic_num_cols + processor.static_cat_cols + processor.dynamic_cat_cols\n",
    "\n",
    "        # n_neighbors = 200\n",
    "        # # Initialize the NearestNeighbors model\n",
    "        # nn_model = NearestNeighbors(n_neighbors=n_neighbors, metric=\"cosine\")\n",
    "        # # Fit the model on the historic data\n",
    "        # nn_model.fit(historic[features_used])\n",
    "\n",
    "        batch_size = 100\n",
    "\n",
    "        num_model, cat_model = processor.train_nn_model_bpic(historic)\n",
    "\n",
    "        for start in range(0, len(current), batch_size):\n",
    "            end = start + batch_size\n",
    "            batch = current.iloc[start:end]\n",
    "            # Find the n nearest neighbors for the selected row\n",
    "            # distances, indices = nn_model.kneighbors([row[features_used]])\n",
    "            distances, indices = processor.find_nearest_neighbors(cat_model, num_model, batch[features_used])\n",
    "            nearest_neighbors = pd.concat([historic.iloc[indices[i]] for i in range(len(batch))])\n",
    "            # nearest_neighbors = historic.iloc[indices.flatten()]\n",
    "\n",
    "            # print(distances.shape)\n",
    "            # print(indices.shape)\n",
    "\n",
    "            # print(cat_model)\n",
    "            # print(num_model)\n",
    "            # print(features_used)\n",
    "            # print(batch[features_used].shape)\n",
    "            print(nearest_neighbors.shape)\n",
    "\n",
    "            target = nearest_neighbors['Target'].values\n",
    "            target_test = batch['Target']\n",
    "\n",
    "            if target_test is None:\n",
    "                continue\n",
    "            \n",
    "\n",
    "            if method == 'Catboost':\n",
    "                \n",
    "                categorical_features_indices = [nearest_neighbors.columns.get_loc(col) for col in processor.static_cat_cols+processor.dynamic_cat_cols]\n",
    "                # Create the CatBoostRegressor model\n",
    "                model = CatBoostClassifier(iterations=100, loss_function='MultiClass', verbose=0, cat_features=processor.static_cat_cols + processor.dynamic_cat_cols) # , max_ctr_complexity=1\n",
    "                print('Now training')\n",
    "                model.fit(nearest_neighbors[features_used], target, cat_features=processor.static_cat_cols + processor.dynamic_cat_cols)\n",
    "\n",
    "            if method == 'HMM':\n",
    "                # Create an instance of the HMM model\n",
    "                model = hmm.GaussianHMM(n_components=7)  # Specify the number of hidden states\n",
    "                model.fit(nearest_neighbors[features_used])\n",
    "\n",
    "            # Make predictions on the testing data\n",
    "            print('Now predicting')\n",
    "            preds = model.predict(batch[features_used])\n",
    "\n",
    "            # Check if array has null values\n",
    "            has_null = pd.Series(preds.flatten(), index=batch.index).isna().any()\n",
    "\n",
    "            if has_null:\n",
    "                print(\"Preds has null values\")\n",
    "\n",
    "\n",
    "            # true_conc_glu = row['Target_orig']\n",
    "            # preds_scaled = scalers[target_col].inverse_transform(preds.reshape(-1, 1))\n",
    "\n",
    "            batch.loc[:, 'predicted_value'] = pd.Series(preds.flatten(), index=batch.index)\n",
    "            is_null = batch['predicted_value'].isna().any()\n",
    "            if is_null:\n",
    "                print(\"Batch has null values\")\n",
    "\n",
    "            results.append(batch)\n",
    "\n",
    "            # Add the current row with its prediction to the historic data\n",
    "            # row_with_prediction[target_col] = preds_scaled[0][0]\n",
    "            historic = pd.concat([historic, batch], ignore_index=True)\n",
    "            historic.sort_values([case_id_col, timestamp_col], ascending=True, kind='mergesort', inplace=True)\n",
    "            nn_model =processor.train_nn_model_bpic(historic)  # Refit the model with the updated historic data\n",
    "\n",
    "        results_df = pd.concat(results)\n",
    "\n",
    "        # Calculate metrics\n",
    "        true_values = results_df[target_col]\n",
    "        predicted_values = results_df['predicted_value']\n",
    "\n",
    "        accuracy = np.mean(true_values == predicted_values)\n",
    "        # MAE_t = mean_absolute_error(true_values, predicted_values)\n",
    "        # MSE_t = mean_squared_error(true_values, predicted_values)\n",
    "        # RMSE_t = math.sqrt(MSE_t)\n",
    "        # r2_t = r2_score(true_values, predicted_values)\n",
    "        # mape_t = mean_absolute_percentage_error(true_values, predicted_values)\n",
    "\n",
    "        # Save results to a CSV file\n",
    "        # results_df.to_csv('predictions_with_row_data.csv', index=False)\n",
    "\n",
    "        # Print metrics\n",
    "        print(f\"Accuracy: {accuracy}\")\n",
    "        # print(f\"MAE: {MAE_t}\")\n",
    "        # print(f\"MSE: {MSE_t}\")\n",
    "        # print(f\"RMSE: {RMSE_t}\")\n",
    "        # print(f\"R2: {r2_t}\")\n",
    "        # print(f\"MAPE: {mape_t}\")\n",
    "\n",
    "        results_df.to_csv(f'results/{dataset_name}/BPIC2011_{method}_{config}_{num_nearest_neighbors}_{distance_metric}.csv', index=False)\n",
    "        print('***********************************')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16b631cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c3e3252",
   "metadata": {},
   "outputs": [],
   "source": [
    "####### Baseline Version without JITL ########\n",
    "\n",
    "# Define the values to experiment with\n",
    "\n",
    "print(f\"Experimenting with num_nearest_neighbors={num_nearest_neighbors} and distance_metric={distance_metric}\")\n",
    "\n",
    "# Create the processor with the current values\n",
    "processor = processor_factory.get_processor(dataset_name, use_encoding=False, use_bucketing=False, num_nearest_neighbors=num_nearest_neighbors, distance_metric=distance_metric)\n",
    "\n",
    "\n",
    "\n",
    "data = df_normalized.sort_values([case_id_col, work_day_col], ascending=True, kind='mergesort')\n",
    "data['Target'] = data.groupby(case_id_col)[target_col].shift(-1)\n",
    "# data['Target_orig'] = data.groupby(case_id_col)['Titer (g/L) original'].shift(-1)\n",
    "data['Target'] = data.groupby(case_id_col)['Target'].ffill().bfill()\n",
    "# data['Target_orig'] = data.groupby(case_id_col)['Target_orig'].ffill()\n",
    "\n",
    "historic, current = processor.split_data(data, train_ratio=0.5, split=\"temporal sim\")\n",
    "historic.sort_values([case_id_col, work_day_col], ascending=True, kind='mergesort', inplace=True)\n",
    "current.sort_values([case_id_col, work_day_col], ascending=True, kind='mergesort', inplace=True)\n",
    "\n",
    "if config == 'no_encoding_bucketing' or config == 'encoding_bucketing':\n",
    "    features_used = features + ['Cluster']\n",
    "else:\n",
    "    features_used = features\n",
    "\n",
    "if method == 'Catboost':\n",
    "        # Create the CatBoostRegressor model\n",
    "        model = CatBoostRegressor(iterations=1000, learning_rate=0.1, depth=6, loss_function='MAE', verbose=0)\n",
    "        model.fit(historic[features_used], target)\n",
    "\n",
    "if method == 'HMM':\n",
    "    # Create an instance of the HMM model\n",
    "    model = hmm.GaussianHMM(n_components=7)  # Specify the number of hidden states\n",
    "    model.fit(historic[features_used])\n",
    "\n",
    "batch_size = 50\n",
    "\n",
    "for start in range(0, len(current), batch_size):\n",
    "    end = start + batch_size\n",
    "    batch = current.iloc[start:end]\n",
    "\n",
    "    target = historic['Target'].values\n",
    "    target_test = batch['Target']\n",
    "\n",
    "    if target_test is None:\n",
    "        continue\n",
    "\n",
    "    # Make predictions on the testing data\n",
    "    preds = model.predict(batch[features_used])\n",
    "\n",
    "    # true_conc_glu = row['Target_orig']\n",
    "    # preds_scaled = scalers[target_col].inverse_transform(preds.reshape(-1, 1))\n",
    "\n",
    "    batch.loc[:, 'predicted_value'] = preds\n",
    "\n",
    "    results.append(batch)\n",
    "\n",
    "\n",
    "results_df = pd.concat(results)\n",
    "\n",
    "# Calculate metrics\n",
    "true_values = results_df[target_col]\n",
    "predicted_values = results_df['predicted_value']\n",
    "\n",
    "MAE_t = mean_absolute_error(true_values, predicted_values)\n",
    "MSE_t = mean_squared_error(true_values, predicted_values)\n",
    "RMSE_t = math.sqrt(MSE_t)\n",
    "r2_t = r2_score(true_values, predicted_values)\n",
    "mape_t = mean_absolute_percentage_error(true_values, predicted_values)\n",
    "\n",
    "# Save results to a CSV file\n",
    "# results_df.to_csv('predictions_with_row_data.csv', index=False)\n",
    "\n",
    "# Print metrics\n",
    "print(f\"MAE: {MAE_t}\")\n",
    "print(f\"MSE: {MSE_t}\")\n",
    "print(f\"RMSE: {RMSE_t}\")\n",
    "print(f\"R2: {r2_t}\")\n",
    "print(f\"MAPE: {mape_t}\")\n",
    "\n",
    "results_df.to_csv(f'results/{dataset_name}/baseline_CSL312_{method}_{config}.csv', index=False)\n",
    "print('***********************************')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aedf2413",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "# Calculate metrics\n",
    "true_values = results_df['true_value']\n",
    "predicted_values = results_df['predicted_value']\n",
    "\n",
    "MAE_t = mean_absolute_error(true_values, predicted_values)\n",
    "MSE_t = mean_squared_error(true_values, predicted_values)\n",
    "RMSE_t = math.sqrt(MSE_t)\n",
    "r2_t = r2_score(true_values, predicted_values)\n",
    "mape_t = mean_absolute_percentage_error(true_values, predicted_values)\n",
    "\n",
    "# Save results to a CSV file\n",
    "# results_df.to_csv('predictions_with_row_data.csv', index=False)\n",
    "\n",
    "# Print metrics\n",
    "print(f\"MAE: {MAE_t}\")\n",
    "print(f\"MSE: {MSE_t}\")\n",
    "print(f\"RMSE: {RMSE_t}\")\n",
    "print(f\"R2: {r2_t}\")\n",
    "print(f\"MAPE: {mape_t}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f104a0fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df.to_csv(f'results/CSL312_{method}_{config}_200_results.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a56260c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import sys\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e405a22d",
   "metadata": {},
   "outputs": [],
   "source": [
    "min_prefix_length = 1\n",
    "max_prefix_length = df_normalized[work_day_col].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5a136d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove both EncoderFactory and AggregateTransformer from sys.modules\n",
    "modules_to_remove = ['EncoderFactory', 'transformers.AggregateTransformer', 'DatasetManager', \"dataset_confs\"]\n",
    "for module in modules_to_remove:\n",
    "    if module in sys.modules:\n",
    "        del sys.modules[module]\n",
    "\n",
    "# Re-import the modules\n",
    "import EncoderFactory\n",
    "import transformers.AggregateTransformer\n",
    "import DatasetManager\n",
    "from DatasetManager import DatasetManager\n",
    "\n",
    "# Reload the modules\n",
    "importlib.reload(transformers.AggregateTransformer)\n",
    "importlib.reload(EncoderFactory)\n",
    "# importlib.reload(DatasetManager)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "521d5525",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_manager = DatasetManager(\"CSL312\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7c4555e",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_test_prefix_generation = time.time()\n",
    "dt_prefixes = dataset_manager.generate_prefix_data(df_normalized, min_prefix_length, max_prefix_length)\n",
    "test_prefix_generation_time = time.time() - start_test_prefix_generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e6cd636",
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_prefixes[target_col].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "683e3507",
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_prefixes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92e55fcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = EncoderFactory.get_encoder(method='agg', case_id_col=case_id_col, static_cat_cols=None, static_num_cols=None, dynamic_cat_cols=['Cluster'],\n",
    "                dynamic_num_cols=features, target_cols=[target_col,'Titer (g/L) original'], work_day_col=work_day_col, fillna=True, max_events=None, activity_col=None, resource_col=None, timestamp_col=timestamp_col,\n",
    "                scale_model=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3d14eb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_transformed = encoder.transform(dt_prefixes)\n",
    "dt_transformed.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95c0a9a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_transformed = dt_transformed.reset_index(drop=False, inplace=False)\n",
    "dt_transformed[work_day_col] = dt_transformed[case_id_col].str.split('_').str[2]\n",
    "dt_transformed[work_day_col] = dt_transformed[work_day_col].fillna(0)\n",
    "dt_transformed[case_id_col] = dt_transformed[case_id_col].str.split('_').str[0] + '_' + dt_transformed[case_id_col].str.split('_').str[1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91018803",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dt_transformed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77ce6cb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_new = [col for col in dt_transformed.columns if col not in [\"Cluster\", \"'Titer (g/L) original'\", case_id_col, target_col, work_day_col]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e8473ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "method = 'Catboost'\n",
    "# config = 'no_encoding_no_bucketing'\n",
    "# config = 'no_encoding_bucketing'\n",
    "config = 'encoding_no_bucketing'\n",
    "# config = 'encoding_bucketing'\n",
    "\n",
    "data = dt_prefixes.sort_values([case_id_col, work_day_col], ascending=True, kind='mergesort')\n",
    "# data['Target'] = data.groupby(case_id_col)[target_col].shift(-1)\n",
    "# data['Target_orig'] = data.groupby(case_id_col)['Titer (g/L) original'].shift(-1)\n",
    "# data['Target'] = data.groupby(case_id_col)['Target'].ffill()\n",
    "# data['Target_orig'] = data.groupby(case_id_col)['Target_orig'].ffill()\n",
    "\n",
    "historic, current = split_data(data, train_ratio=0.5, split=\"temporal sim\")\n",
    "historic.sort_values([case_id_col, work_day_col], ascending=True, kind='mergesort', inplace=True)\n",
    "current.sort_values([case_id_col, work_day_col], ascending=True, kind='mergesort', inplace=True)\n",
    "\n",
    "if config == 'no_encoding_bucketing' or config == 'encoding_bucketing':\n",
    "    features_used = features + ['Cluster']\n",
    "else:\n",
    "    features_used = features\n",
    "\n",
    "# Initialize the NearestNeighbors model\n",
    "nn_model = NearestNeighbors(n_neighbors=200)\n",
    "# Fit the model on the historic data\n",
    "nn_model.fit(historic[features_used])\n",
    "\n",
    "for index, row in current.iterrows():\n",
    "    # Find the n nearest neighbors for the selected row\n",
    "    distances, indices = nn_model.kneighbors([row[features_used]])\n",
    "    nearest_neighbors = historic.iloc[indices[0]]\n",
    "\n",
    "    target = nearest_neighbors['Target'].values\n",
    "    target_test = row['Target']\n",
    "\n",
    "    if target_test is None:\n",
    "        continue\n",
    "    \n",
    "\n",
    "    if method == 'Catboost':\n",
    "        # Create the CatBoostRegressor model\n",
    "        model = CatBoostRegressor(iterations=1000, learning_rate=0.1, depth=6, loss_function='MAE', verbose=0)\n",
    "        model.fit(nearest_neighbors[features_used], target)\n",
    "\n",
    "    if method == 'HMM':\n",
    "        # Create an instance of the HMM model\n",
    "        model = hmm.GaussianHMM(n_components=7)  # Specify the number of hidden states\n",
    "        model.fit(df_without_last[features+['Cluster']])\n",
    "\n",
    "    # Make predictions on the testing data\n",
    "    preds = model.predict(row[features_used])\n",
    "\n",
    "    true_conc_glu = row['Target_orig']\n",
    "    preds_scaled = scalers[target_col].inverse_transform(preds.reshape(-1, 1))\n",
    "    \n",
    "    key = row[case_id_col] + '_' + str(row[work_day_col])\n",
    "    results.append({\n",
    "        'key': key,\n",
    "        'row_data': row.to_dict(),\n",
    "        'true_value': true_conc_glu,\n",
    "        'predicted_value': preds_scaled[0][0]\n",
    "    })\n",
    "\n",
    "    # Add the current row with its prediction to the historic data\n",
    "    row_with_prediction = row.copy()\n",
    "    # row_with_prediction[target_col] = preds_scaled[0][0]\n",
    "    historic = pd.concat([historic, pd.DataFrame([row_with_prediction])], ignore_index=True)\n",
    "    historic.sort_values([case_id_col, work_day_col], ascending=True, kind='mergesort', inplace=True)\n",
    "    nn_model.fit(historic[features_used])  # Refit the model with the updated historic data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0db896ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "# Calculate metrics\n",
    "true_values = results_df['true_value']\n",
    "predicted_values = results_df['predicted_value']\n",
    "\n",
    "MAE_t = mean_absolute_error(true_values, predicted_values)\n",
    "MSE_t = mean_squared_error(true_values, predicted_values)\n",
    "RMSE_t = math.sqrt(MSE_t)\n",
    "r2_t = r2_score(true_values, predicted_values)\n",
    "mape_t = mean_absolute_percentage_error(true_values, predicted_values)\n",
    "\n",
    "# Save results to a CSV file\n",
    "# results_df.to_csv('predictions_with_row_data.csv', index=False)\n",
    "\n",
    "# Print metrics\n",
    "print(f\"MAE: {MAE_t}\")\n",
    "print(f\"MSE: {MSE_t}\")\n",
    "print(f\"RMSE: {RMSE_t}\")\n",
    "print(f\"R2: {r2_t}\")\n",
    "print(f\"MAPE: {mape_t}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36315cb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df.to_csv(f'results/CSL312_{method}_{config}_results.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0653ada2",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "# Calculate metrics\n",
    "true_values = results_df['true_value']\n",
    "predicted_values = results_df['predicted_value']\n",
    "\n",
    "MAE_t = mean_absolute_error(true_values, predicted_values)\n",
    "MSE_t = mean_squared_error(true_values, predicted_values)\n",
    "RMSE_t = math.sqrt(MSE_t)\n",
    "r2_t = r2_score(true_values, predicted_values)\n",
    "mape_t = mean_absolute_percentage_error(true_values, predicted_values)\n",
    "\n",
    "# Save results to a CSV file\n",
    "# results_df.to_csv('predictions_with_row_data.csv', index=False)\n",
    "\n",
    "# Print metrics\n",
    "print(f\"MAE: {MAE_t}\")\n",
    "print(f\"MSE: {MSE_t}\")\n",
    "print(f\"RMSE: {RMSE_t}\")\n",
    "print(f\"R2: {r2_t}\")\n",
    "print(f\"MAPE: {mape_t}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf543394",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e74e4fa",
   "metadata": {},
   "source": [
    "#################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25c58223",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "method = 'Catboost'\n",
    "config = 'no_encoding_no_bucketing'\n",
    "# config = 'no_encoding_bucketing'\n",
    "# config = 'encoding_no_bucketing'\n",
    "# config = 'encoding_bucketing'\n",
    "\n",
    "data = df_normalized.sort_values([case_id_col, work_day_col], ascending=True, kind='mergesort')\n",
    "data['Target'] = data.groupby(case_id_col)[target_col].shift(-1)\n",
    "data['Target_orig'] = data.groupby(case_id_col)['Titer (g/L) original'].shift(-1)\n",
    "data['Target'] = data.groupby(case_id_col)['Target'].ffill().bfill()\n",
    "data['Target_orig'] = data.groupby(case_id_col)['Target_orig'].ffill().bfill()\n",
    "\n",
    "historic, current = split_data(data, train_ratio=0.5, split=\"temporal sim\")\n",
    "historic.sort_values([case_id_col, work_day_col], ascending=True, kind='mergesort', inplace=True)\n",
    "current.sort_values([case_id_col, work_day_col], ascending=True, kind='mergesort', inplace=True)\n",
    "\n",
    "if config == 'no_encoding_bucketing' or config == 'encoding_bucketing':\n",
    "    features_used = features + ['Cluster']\n",
    "else:\n",
    "    features_used = features\n",
    "\n",
    "\n",
    "for index, row in current.iterrows():\n",
    "\n",
    "    target = historic['Target'].values\n",
    "    target_test = row['Target']\n",
    "\n",
    "    if target_test is None:\n",
    "        continue\n",
    "    \n",
    "\n",
    "    if method == 'Catboost':\n",
    "        # Create the CatBoostRegressor model\n",
    "        model = CatBoostRegressor(iterations=1000, learning_rate=0.1, depth=6, loss_function='MAE', verbose=0, random_state=123)\n",
    "        model.fit(historic[features_used], target)\n",
    "\n",
    "    if method == 'HMM':\n",
    "        # Create an instance of the HMM model\n",
    "        model = hmm.GaussianHMM(n_components=7)  # Specify the number of hidden states\n",
    "        model.fit(df_without_last[features+['Cluster']])\n",
    "\n",
    "    # Make predictions on the testing data\n",
    "    preds = model.predict(row[features_used])\n",
    "\n",
    "    true_conc_glu = row['Target_orig']\n",
    "    preds_scaled = scalers[target_col].inverse_transform(preds.reshape(-1, 1))\n",
    "    \n",
    "    key = row[case_id_col] + '_' + str(row[work_day_col])\n",
    "    results.append({\n",
    "        'key': key,\n",
    "        'row_data': row.to_dict(),\n",
    "        'true_value': true_conc_glu,\n",
    "        'predicted_value': preds_scaled[0][0]\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bdacadb",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "# Calculate metrics\n",
    "true_values = results_df['true_value']\n",
    "predicted_values = results_df['predicted_value']\n",
    "\n",
    "MAE_t = mean_absolute_error(true_values, predicted_values)\n",
    "MSE_t = mean_squared_error(true_values, predicted_values)\n",
    "RMSE_t = math.sqrt(MSE_t)\n",
    "r2_t = r2_score(true_values, predicted_values)\n",
    "mape_t = mean_absolute_percentage_error(true_values, predicted_values)\n",
    "\n",
    "# Save results to a CSV file\n",
    "# results_df.to_csv('predictions_with_row_data.csv', index=False)\n",
    "\n",
    "# Print metrics\n",
    "print(f\"MAE: {MAE_t}\")\n",
    "print(f\"MSE: {MSE_t}\")\n",
    "print(f\"RMSE: {RMSE_t}\")\n",
    "print(f\"R2: {r2_t}\")\n",
    "print(f\"MAPE: {mape_t}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0834eeb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df.to_csv(f'results/CSL312_baseline_{method}_{config}_results.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "304ff943",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df_400 = pd.read_csv('results/CSL312_Catboost_no_encoding_bucketing_cosine_results.csv')\n",
    "results_df_300 = pd.read_csv('results/CSL312_Catboost_no_encoding_bucketing_chebyshev_results.csv')\n",
    "results_df_200 = pd.read_csv('results/CSL312_Catboost_no_encoding_bucketing_200_results.csv')\n",
    "results_df_100 = pd.read_csv('results/CSL312_Catboost_no_encoding_bucketing_DTW_results.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8193f25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the row_data column\n",
    "row_data_df = pd.json_normalize(results_df_400['row_data'])\n",
    "\n",
    "# Drop the original row_data column from results_df_baseline\n",
    "tmp = results_df_400.drop(columns=['row_data'])\n",
    "\n",
    "# Concatenate the normalized row_data DataFrame with the original DataFrame\n",
    "tmp = pd.concat([tmp, row_data_df], axis=1)\n",
    "\n",
    "# Retain true_value and predicted_value columns\n",
    "tmp = tmp[['true_value', 'predicted_value'] + list(row_data_df.columns)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "511ef84c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the row_data column\n",
    "row_data_df = pd.json_normalize(results_df_300['row_data'])\n",
    "\n",
    "# Drop the original row_data column from results_df_baseline\n",
    "tmp2 = results_df_300.drop(columns=['row_data'])\n",
    "\n",
    "# Concatenate the normalized row_data DataFrame with the original DataFrame\n",
    "tmp2 = pd.concat([tmp2, row_data_df], axis=1)\n",
    "\n",
    "# Retain true_value and predicted_value columns\n",
    "tmp2 = tmp2[['true_value', 'predicted_value'] + list(row_data_df.columns)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa33d4bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the row_data column\n",
    "row_data_df = pd.json_normalize(results_df_200['row_data'])\n",
    "\n",
    "# Drop the original row_data column from results_df_baseline\n",
    "tmp3 = results_df_200.drop(columns=['row_data'])\n",
    "\n",
    "# Concatenate the normalized row_data DataFrame with the original DataFrame\n",
    "tmp3 = pd.concat([tmp3, row_data_df], axis=1)\n",
    "\n",
    "# Retain true_value and predicted_value columns\n",
    "tmp3 = tmp3[['true_value', 'predicted_value'] + list(row_data_df.columns)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0eb23db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the row_data column\n",
    "row_data_df = pd.json_normalize(results_df_100['row_data'])\n",
    "\n",
    "# Drop the original row_data column from results_df_baseline\n",
    "tmp4 = results_df_100.drop(columns=['row_data'])\n",
    "\n",
    "# Concatenate the normalized row_data DataFrame with the original DataFrame\n",
    "tmp4 = pd.concat([tmp4, row_data_df], axis=1)\n",
    "\n",
    "# Retain true_value and predicted_value columns\n",
    "tmp4 = tmp4[['true_value', 'predicted_value'] + list(row_data_df.columns)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d090dc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_moving_avg_mae(df, true_col, pred_col, window_size=5):\n",
    "    true_values = df[true_col].to_numpy()\n",
    "    predicted_values = df[pred_col].to_numpy()\n",
    "\n",
    "    num_rows_list = []\n",
    "    mae_list = []\n",
    "\n",
    "    for i in range(2, len(true_values) + 1):\n",
    "        num_rows_list.append(i)\n",
    "        mae = mean_absolute_error(true_values[:i], predicted_values[:i])\n",
    "        mae_list.append(mae)\n",
    "\n",
    "    mae_df = pd.DataFrame({'num_rows': num_rows_list, 'mae': mae_list})\n",
    "    mae_df['moving_avg_mae'] = mae_df['mae'].rolling(window=window_size).mean()\n",
    "    \n",
    "    return mae_df\n",
    "\n",
    "# Calculate moving average MAE for tmp\n",
    "mae_df_tmp = calculate_moving_avg_mae(tmp, 'true_value', 'predicted_value')\n",
    "\n",
    "# Calculate moving average MAE for tmp2\n",
    "mae_df_tmp2 = calculate_moving_avg_mae(tmp2, 'true_value', 'predicted_value')\n",
    "\n",
    "# Calculate moving average MAE for tmp2\n",
    "mae_df_tmp3 = calculate_moving_avg_mae(tmp3, 'true_value', 'predicted_value')\n",
    "\n",
    "# Calculate moving average MAE for tmp2\n",
    "mae_df_tmp4 = calculate_moving_avg_mae(tmp4, 'true_value', 'predicted_value')\n",
    "\n",
    "# Plot the moving average MAE for both DataFrames\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(mae_df_tmp['num_rows'], mae_df_tmp['moving_avg_mae'], label='cosine', color='blue')\n",
    "plt.plot(mae_df_tmp2['num_rows'], mae_df_tmp2['moving_avg_mae'], label='levenshtein', color='red')\n",
    "plt.plot(mae_df_tmp3['num_rows'], mae_df_tmp3['moving_avg_mae'], label='euclidean', color='green')\n",
    "plt.plot(mae_df_tmp4['num_rows'], mae_df_tmp4['moving_avg_mae'], label='DTW', color='black')\n",
    "plt.xlabel('Number of Observed Rows')\n",
    "plt.xlabel('Number of Observed Rows')\n",
    "plt.ylabel('Moving Average MAE')\n",
    "plt.title('CSL312')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "758141cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results = pd.DataFrame(results).T\n",
    "# catboost regular \n",
    "pd.options.display.float_format = '{:.2f}'.format\n",
    "df_results[df_results.columns].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "636cc063",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results = pd.DataFrame(results).T\n",
    "# HMM regular \n",
    "pd.options.display.float_format = '{:.2f}'.format\n",
    "df_results[df_results.columns].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d19d0cb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
