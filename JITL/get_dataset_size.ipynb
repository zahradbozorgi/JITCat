{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5a683265-5556-4ea9-b537-477150acee41",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import seaborn as sns\n",
    "\n",
    "from pathlib import Path\n",
    "#from torch import cdist\n",
    "import os\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fc01ccd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from catboost import CatBoostClassifier\n",
    "# from hmmlearn import hmm\n",
    "import sklearn \n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, f1_score, confusion_matrix, precision_score, recall_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler, PolynomialFeatures\n",
    "from sklearn.pipeline import make_pipeline \n",
    "from sklearn.compose import make_column_selector, make_column_transformer\n",
    "\n",
    "# import math\n",
    "# from statsmodels.tsa.arima.model import ARIMA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d6efa3ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import category_encoders as ce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c4292a19",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "38ab6a18",
   "metadata": {},
   "outputs": [],
   "source": [
    "ROOT_DIR = Path().cwd()\n",
    "while not ROOT_DIR.joinpath(\"data\").exists():\n",
    "    ROOT_DIR = ROOT_DIR.parent\n",
    "os.chdir(ROOT_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "290bd199",
   "metadata": {},
   "outputs": [],
   "source": [
    "from processors import processor_factory\n",
    "from data.DatasetManager import DatasetManager\n",
    "import data.EncoderFactory as EncoderFactory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1807370c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_first_n_cases(df, n):\n",
    "    earliest_timestamps = df.groupby(dataset_manager.case_id_col)[dataset_manager.timestamp_col].min()\n",
    "    sorted_cases = earliest_timestamps.sort_values().index[:n]\n",
    "    return df[df[dataset_manager.case_id_col].isin(sorted_cases)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ef5b5dce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_last_row(group):\n",
    "    max_event_row = group.loc[group['event_nr'].idxmax()]\n",
    "    return max_event_row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fd096c8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['bpic2011_f1', 'bpic2011_f2', 'bpic2011_f3', 'bpic2011_f4', 'bpic2015_1_f2', 'bpic2015_2_f2', 'bpic2015_3_f2', 'bpic2015_4_f2', 'bpic2015_5_f2', 'bpic2012_cancelled', 'sepsis_cases_1', 'sepsis_cases_2', 'sepsis_cases_4', 'bpic2012_accepted', 'bpic2012_declined', 'bpic2017_accepted', 'bpic2017_cancelled', 'bpic2017_refused', 'production', 'traffic_fines_1']\n"
     ]
    }
   ],
   "source": [
    "dataset_ref = [\"bpic2011\",\"bpic2015\",  \"bpic2012_cancelled\", \"sepsis_cases\", \"bpic2012_accepted\", \"bpic2012_declined\", 'bpic2017_accepted', 'bpic2017_cancelled', 'bpic2017_refused', 'production', 'traffic_fines_1']\n",
    "dataset_ref_to_datasets = {\n",
    "    \"bpic2011\": [\"bpic2011_f%s\" % formula for formula in range(1, 5)],\n",
    "    \"bpic2015\": [\"bpic2015_%s_f2\" % (municipality) for municipality in range(1, 6)],\n",
    "    \"sepsis_cases\": [\"sepsis_cases_1\", \"sepsis_cases_2\", \"sepsis_cases_4\"]\n",
    "}\n",
    "\n",
    "# Check if any element in dataset_ref is a key in dataset_ref_to_datasets\n",
    "datasets = []\n",
    "for ref in dataset_ref:\n",
    "    if ref in dataset_ref_to_datasets:\n",
    "        datasets.extend(dataset_ref_to_datasets[ref])\n",
    "    else:\n",
    "        datasets.append(ref)\n",
    "\n",
    "print(datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1209af21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of cases in bpic2011_f1: 1140\n",
      "Generating prefix data...\n",
      "Activity code\n",
      "(24176, 229)\n",
      "Number of cases in bpic2011_f2: 1140\n",
      "Generating prefix data...\n",
      "Activity code\n",
      "(31235, 265)\n",
      "Number of cases in bpic2011_f3: 1121\n",
      "Generating prefix data...\n",
      "Activity code\n",
      "(20534, 223)\n",
      "Number of cases in bpic2011_f4: 1140\n",
      "Generating prefix data...\n",
      "Activity code\n",
      "(30928, 253)\n",
      "Number of cases in bpic2015_1_f2: 696\n",
      "Generating prefix data...\n",
      "Activity\n",
      "(23786, 201)\n",
      "Number of cases in bpic2015_2_f2: 753\n",
      "Generating prefix data...\n",
      "Activity\n",
      "(28377, 205)\n",
      "Number of cases in bpic2015_3_f2: 1328\n",
      "Generating prefix data...\n",
      "Activity\n",
      "(48333, 212)\n",
      "Number of cases in bpic2015_4_f2: 577\n",
      "Generating prefix data...\n",
      "Activity\n",
      "(20464, 163)\n",
      "Number of cases in bpic2015_5_f2: 1051\n",
      "Generating prefix data...\n",
      "Activity\n",
      "(40530, 213)\n",
      "Number of cases in bpic2012_cancelled: 4685\n",
      "Generating prefix data...\n",
      "Activity\n",
      "(155783, 144)\n",
      "Number of cases in sepsis_cases_1: 782\n",
      "Generating prefix data...\n",
      "Activity\n",
      "(12189, 122)\n",
      "Number of cases in sepsis_cases_2: 782\n",
      "Generating prefix data...\n",
      "Activity\n",
      "(9178, 121)\n",
      "Number of cases in sepsis_cases_4: 782\n",
      "Generating prefix data...\n",
      "Activity\n",
      "(11056, 122)\n",
      "Number of cases in bpic2012_accepted: 4685\n",
      "Generating prefix data...\n",
      "Activity\n",
      "(155783, 144)\n",
      "Number of cases in bpic2012_declined: 4685\n",
      "Generating prefix data...\n",
      "Activity\n",
      "(155783, 144)\n",
      "Number of cases in bpic2017_accepted: 31413\n",
      "Generating prefix data...\n",
      "Activity\n",
      "(624352, 267)\n",
      "Number of cases in bpic2017_cancelled: 31413\n",
      "Generating prefix data...\n",
      "Activity\n",
      "(624352, 267)\n",
      "Number of cases in bpic2017_refused: 31413\n",
      "Generating prefix data...\n",
      "Activity\n",
      "(624352, 267)\n",
      "Number of cases in production: 220\n",
      "Generating prefix data...\n",
      "Activity\n",
      "(2275, 128)\n",
      "Number of cases in traffic_fines_1: 129615\n",
      "Generating prefix data...\n",
      "Activity\n",
      "(460462, 229)\n",
      "         dataset_name num_rows num_columns\n",
      "0         bpic2011_f1    24176         229\n",
      "1         bpic2011_f2    31235         265\n",
      "2         bpic2011_f3    20534         223\n",
      "3         bpic2011_f4    30928         253\n",
      "4       bpic2015_1_f2    23786         197\n",
      "5       bpic2015_2_f2    28377         201\n",
      "6       bpic2015_3_f2    48333         208\n",
      "7       bpic2015_4_f2    20464         159\n",
      "8       bpic2015_5_f2    40530         209\n",
      "9  bpic2012_cancelled   155783         139\n"
     ]
    }
   ],
   "source": [
    "# Initialize a DataFrame to store dataset names and shapes\n",
    "dataset_shapes = pd.DataFrame(columns=['dataset_name', 'num_rows', 'num_columns'])\n",
    "transformed_shapes = pd.DataFrame(columns=['dataset_name', 'num_rows', 'num_columns'])\n",
    "\n",
    "for dataset_name in datasets:\n",
    "    dataset_manager = DatasetManager(dataset_name)\n",
    "    df = dataset_manager.read_dataset()\n",
    "    df.sort_values([dataset_manager.case_id_col, dataset_manager.timestamp_col], inplace=True)\n",
    "    print(f\"Number of cases in {dataset_name}: {df[dataset_manager.case_id_col].nunique()}\")\n",
    "    max_case_num = df[dataset_manager.case_id_col].nunique()\n",
    "    filtered_df = get_first_n_cases(df, max_case_num)\n",
    "    for col in [dataset_manager.activity_col]:\n",
    "        counts = filtered_df[col].value_counts()\n",
    "        mask = filtered_df[col].isin(counts[counts >= 100].index)\n",
    "        filtered_df.loc[~mask, col] = \"other\"\n",
    "\n",
    "    # Determine min and max (truncated) prefix lengths\n",
    "    min_prefix_length = 1\n",
    "    if \"traffic_fines\" in dataset_name:\n",
    "        max_prefix_length = 10\n",
    "    elif \"bpic2017\" in dataset_name:\n",
    "        max_prefix_length = min(20, dataset_manager.get_pos_case_length_quantile(filtered_df, 0.90))\n",
    "    else:\n",
    "        max_prefix_length = min(40, dataset_manager.get_pos_case_length_quantile(filtered_df, 0.90))\n",
    "\n",
    "    start_test_prefix_generation = time.time()\n",
    "    print(\"Generating prefix data...\")\n",
    "    dt_prefixes = dataset_manager.generate_prefix_data(filtered_df, min_prefix_length, max_prefix_length)\n",
    "    test_prefix_generation_time = time.time() - start_test_prefix_generation\n",
    "\n",
    "    encoder = EncoderFactory.get_encoder(method='agg', case_id_col=dataset_manager.case_id_col, static_cat_cols=dataset_manager.static_cat_cols, \n",
    "                                     static_num_cols=dataset_manager.static_num_cols, dynamic_cat_cols=dataset_manager.dynamic_cat_cols,\n",
    "                                     dynamic_num_cols=dataset_manager.dynamic_num_cols, fillna=True, max_events=None, \n",
    "                                     activity_col=dataset_manager.activity_col, resource_col=None, \n",
    "                                     timestamp_col=dataset_manager.timestamp_col, scale_model=None)\n",
    "    \n",
    "    dt_transformed = encoder.transform(dt_prefixes)\n",
    "    dt_transformed.reset_index(drop=False, inplace=False)[dataset_manager.case_id_col]\n",
    "\n",
    "    subset = dt_prefixes[[dataset_manager.case_id_col, dataset_manager.timestamp_col, dataset_manager.activity_col, dataset_manager.label_col, 'event_nr', 'case_length'] + dataset_manager.static_num_cols+dataset_manager.static_cat_cols]\n",
    "    subset = subset.groupby(dataset_manager.case_id_col).apply(return_last_row).reset_index(drop=True)\n",
    "    # Create a new column 'finished' with values based on the condition\n",
    "    subset['finished'] = (subset['event_nr'] == subset['case_length']).astype(int)\n",
    "    \n",
    "    merged_df = pd.merge(subset, dt_transformed, on=[dataset_manager.case_id_col])\n",
    "\n",
    "    # Step 1: Identify object columns\n",
    "    object_columns = merged_df.select_dtypes(include=['object']).columns\n",
    "\n",
    "    # Step 2: Check if object columns contain boolean values\n",
    "    for col in object_columns:\n",
    "        if merged_df[col].isin(['True', 'False', 'TRUE', 'FALSE', 'true', 'false']).all():\n",
    "            merged_df[col] = merged_df[col].str.lower().map({'true': True, 'false': False})\n",
    "\n",
    "            # Step 3: Transform boolean object columns to boolean data type\n",
    "            merged_df[col] = merged_df[col].astype('boolean')\n",
    "\n",
    "    # Add dataset name and shape of merged_df to the dataset_shapes DataFrame\n",
    "    dataset_shapes = pd.concat([dataset_shapes, pd.DataFrame({\n",
    "        'dataset_name': [dataset_name],\n",
    "        'num_rows': [merged_df.shape[0]],\n",
    "        'num_columns': [merged_df.shape[1]]\n",
    "    })], ignore_index=True)\n",
    "\n",
    "    print(merged_df.shape)\n",
    "\n",
    "    preprocessor = make_column_transformer(\n",
    "        (StandardScaler(), make_column_selector(dtype_include=['int64', 'float64'])),  # Scale continuous variables\n",
    "        (ce.quantile_encoder.SummaryEncoder(), make_column_selector(dtype_include=['object', 'category'])),  # Encode categorical variables\n",
    "        remainder='drop')\n",
    "    \n",
    "    features_used = merged_df.columns.difference([dataset_manager.label_col, dataset_manager.timestamp_col, dataset_manager.case_id_col, 'event_nr', 'case_length', 'finished'], sort=False)\n",
    "    merged_df_transformed = preprocessor.fit_transform(merged_df[features_used], merged_df[dataset_manager.label_col])\n",
    "\n",
    "    transformed_shapes = pd.concat([transformed_shapes, pd.DataFrame({\n",
    "        'dataset_name': [dataset_name],\n",
    "        'num_rows': [merged_df_transformed.shape[0]],\n",
    "        'num_columns': [merged_df_transformed.shape[1]]\n",
    "    })], ignore_index=True)\n",
    "\n",
    "# Print the final DataFrame with dataset names and shapes\n",
    "# print(dataset_shapes.head(10))\n",
    "print(transformed_shapes.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b512274a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_shapes.to_csv(\"dataset_sizes.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e7843567",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed_shapes.to_csv(\"transformed_dataset_sizes.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3b173b67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of cases in bpic2011_f1: 1140\n",
      "Number of cases in bpic2011_f2: 1140\n",
      "Number of cases in bpic2011_f3: 1121\n",
      "Number of cases in bpic2011_f4: 1140\n",
      "Number of cases in bpic2015_1_f2: 696\n",
      "Number of cases in bpic2015_2_f2: 753\n",
      "Number of cases in bpic2015_3_f2: 1328\n",
      "Number of cases in bpic2015_4_f2: 577\n",
      "Number of cases in bpic2015_5_f2: 1051\n",
      "Number of cases in bpic2012_cancelled: 4685\n",
      "Number of cases in sepsis_cases_1: 782\n",
      "Number of cases in sepsis_cases_2: 782\n",
      "Number of cases in sepsis_cases_4: 782\n",
      "Number of cases in bpic2012_accepted: 4685\n",
      "Number of cases in bpic2012_declined: 4685\n",
      "Number of cases in bpic2017_accepted: 31413\n",
      "Number of cases in bpic2017_cancelled: 31413\n",
      "Number of cases in bpic2017_refused: 31413\n",
      "Number of cases in production: 220\n",
      "Number of cases in traffic_fines_1: 129615\n"
     ]
    }
   ],
   "source": [
    "# Initialize a DataFrame to store dataset names and shapes\n",
    "# dataset_shapes = pd.DataFrame(columns=['dataset_name', 'num_rows', 'num_columns'])\n",
    "# transformed_shapes = pd.DataFrame(columns=['dataset_name', 'num_rows', 'num_columns'])\n",
    "results = []\n",
    "for dataset_name in datasets:\n",
    "    dataset_manager = DatasetManager(dataset_name)\n",
    "    df = dataset_manager.read_dataset()\n",
    "    df.sort_values([dataset_manager.case_id_col, dataset_manager.timestamp_col], inplace=True)\n",
    "    print(f\"Number of cases in {dataset_name}: {df[dataset_manager.case_id_col].nunique()}\")\n",
    "    max_case_num = df[dataset_manager.case_id_col].nunique()\n",
    "    filtered_df = get_first_n_cases(df, max_case_num)\n",
    "    \n",
    "    # Group by case_id_col and find the dominant class for each case_id\n",
    "    case_class = (\n",
    "        filtered_df.groupby(dataset_manager.case_id_col)[dataset_manager.label_col]\n",
    "        .agg(lambda x: x.mode()[0])  # Get the most frequent (mode) class for each case_id\n",
    "    )\n",
    "    \n",
    "    # Count how many case_ids are \"regular\" and how many are \"deviant\"\n",
    "    regular_count = (case_class == \"regular\").sum()\n",
    "    deviant_count = (case_class == \"deviant\").sum()\n",
    "    \n",
    "    # Compute proportions\n",
    "    total_count = regular_count + deviant_count\n",
    "    p_regular = regular_count / total_count\n",
    "    p_deviant = deviant_count / total_count\n",
    "    \n",
    "    # Compute metrics\n",
    "    imbalance_ratio = max(regular_count, deviant_count) / min(regular_count, deviant_count)\n",
    "    gini_index = 1 - (p_regular**2 + p_deviant**2)\n",
    "    majority_to_minority_ratio = max(regular_count, deviant_count) / min(regular_count, deviant_count)\n",
    "    \n",
    "    # Append results to the list\n",
    "    results.append({\n",
    "        \"dataset_name\": dataset_name,\n",
    "        \"imbalance_ratio\": imbalance_ratio,\n",
    "        \"gini_index\": gini_index,\n",
    "        \"majority_to_minority_ratio\": majority_to_minority_ratio\n",
    "    })\n",
    "\n",
    "# Convert results to a DataFrame\n",
    "results_df = pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "bfb4f44a",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df.to_csv(\"dataset_imbalance_metrics.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be873643",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_threshold(proba_values, true_values):\n",
    "    list_acc = []\n",
    "    thresholds = np.arange(0, 1.0, 0.05)\n",
    "    true_values = true_values.map({'regular': True, 'deviant': False})\n",
    "\n",
    "\n",
    "    for threshold in thresholds:\n",
    "        preds_thr = proba_values > threshold\n",
    "        acc= accuracy_score(true_values, preds_thr)\n",
    "        # acc = np.mean(true_values == preds_thr)\n",
    "        list_acc.append(acc)\n",
    "        \n",
    "        #print(f\"Threshold: {threshold}, Accuracy: {acc}\")\n",
    "\n",
    "    # plt.plot(thresholds, list_acc)\n",
    "\n",
    "\n",
    "    best_threshold = thresholds[np.where(list_acc==np.max(list_acc))]\n",
    "    best_accuracy = np.max(list_acc)\n",
    "\n",
    "    # print(f\"Best Threshold: {best_threshold}, Best Accuracy: {best_accuracy}\")\n",
    "    return best_threshold, best_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b817b22",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "method = 'Catboost'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26ef6e2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the values to experiment with\n",
    "results = []\n",
    "trainingtimes = []\n",
    "AUCs = []\n",
    "num_nearest_neighbors_values = [300]\n",
    "distance_metrics = ['euclidean']\n",
    "results_df = pd.DataFrame()\n",
    "\n",
    "# Loop through the values and create processors\n",
    "for num_nearest_neighbors in num_nearest_neighbors_values:\n",
    "    for distance_metric in distance_metrics:\n",
    "        print(f\"Experimenting with num_nearest_neighbors={num_nearest_neighbors} and distance_metric={distance_metric}\")\n",
    "        \n",
    "        # Create the processor with the current values\n",
    "        processor = processor_factory.get_processor(dataset_name, use_encoding=False, use_bucketing=False, num_nearest_neighbors=num_nearest_neighbors, distance_metric=distance_metric)\n",
    "\n",
    "        data = merged_df.sort_values([dataset_manager.case_id_col, dataset_manager.timestamp_col], ascending=True, kind='mergesort')\n",
    "        historic, current = processor.split_data_strict(data, train_ratio=0.5)\n",
    "        historic.sort_values([dataset_manager.timestamp_col], ascending=True, kind='mergesort', inplace=True)\n",
    "        current.sort_values([dataset_manager.timestamp_col], ascending=True, kind='mergesort', inplace=True)\n",
    "        # current = current.head(1000)\n",
    "\n",
    "\n",
    "        # Exclude specific columns from historic\n",
    "        features_used = historic.columns.difference([dataset_manager.label_col, dataset_manager.timestamp_col, dataset_manager.case_id_col, 'event_nr', 'case_length', 'finished'], sort=False)\n",
    "        # n_neighbors = 200\n",
    "        # # Initialize the NearestNeighbors model\n",
    "        # nn_model = NearestNeighbors(n_neighbors=n_neighbors, metric=\"cosine\")\n",
    "        # # Fit the model on the historic data\n",
    "        # nn_model.fit(historic[features_used])\n",
    "\n",
    "        batch_size = 100\n",
    "\n",
    "        initial_start_time = time.time()\n",
    "\n",
    "        # Define the preprocessor\n",
    "        preprocessor = make_column_transformer(\n",
    "        (StandardScaler(), make_column_selector(dtype_include=['int64', 'float64'])),  # Scale continuous variables\n",
    "        (ce.quantile_encoder.SummaryEncoder(), make_column_selector(dtype_include=['object', 'category'])),  # Encode categorical variables\n",
    "        remainder='drop')  # Drop other columns\n",
    "\n",
    "        # num_model, cat_model = processor.train_nn_model_bpic_encoding(historic[features_used])\n",
    "        nn_model = NearestNeighbors(n_neighbors=num_nearest_neighbors, metric=distance_metric)\n",
    "        # nn_model = make_pipeline(preprocessor,('nearestneighbors', \n",
    "        #                         NearestNeighbors(n_neighbors=num_nearest_neighbors, metric=distance_metric)))\n",
    "\n",
    "        historic_transformed = preprocessor.fit_transform(historic[features_used], historic[dataset_manager.label_col])\n",
    "        nn_model.fit(historic_transformed)\n",
    "        # nn_model.fit(historic.drop(columns=[dataset_manager.label_col, dataset_manager.timestamp_col, dataset_manager.case_id_col, dataset_manager.activity_col]))\n",
    "\n",
    "\n",
    "\n",
    "        initial_end_time = time.time()\n",
    "\n",
    "        initial_training_time = initial_end_time - initial_start_time\n",
    "        print(\"Initial Model Training time: \", initial_training_time, \" seconds\")\n",
    "\n",
    "        for start in range(0, len(current), batch_size):\n",
    "            end = start + batch_size\n",
    "            batch = current.iloc[start:end]\n",
    "            # Find the n nearest neighbors for the selected row\n",
    "            #distances, indices = nn_model.kneighbors(batch.drop(columns=[dataset_manager.label_col, dataset_manager.timestamp_col, dataset_manager.case_id_col, dataset_manager.activity_col]).to_numpy())\n",
    "            # Drop non-numeric columns from the batch DataFrame\n",
    "            # numeric_batch = batch.select_dtypes(include=['number'])\n",
    "\n",
    "            distances, indices = nn_model.kneighbors(preprocessor.transform(batch[features_used]))\n",
    "            # distances, indices = nn_model[1].kneighbors(batch.drop(columns=[dataset_manager.label_col, dataset_manager.timestamp_col, dataset_manager.case_id_col, dataset_manager.activity_col]).to_numpy())\n",
    "            # distances, indices = processor.find_nearest_neighbors_encoding(cat_model, num_model, batch[features_used])\n",
    "            nearest_neighbors = pd.concat([historic.iloc[indices[i]] for i in range(len(batch))])\n",
    "            # nearest_neighbors = historic.iloc[indices.flatten()]\n",
    "\n",
    "            # print(distances.shape)\n",
    "            # print(indices.shape)\n",
    "\n",
    "            # print(cat_model)\n",
    "            # print(num_model)\n",
    "            # print(features_used)\n",
    "            # print(batch[features_used].shape)\n",
    "            print(nearest_neighbors.shape)\n",
    "\n",
    "            target = nearest_neighbors[dataset_manager.label_col].values\n",
    "            target_test = batch[dataset_manager.label_col]\n",
    "\n",
    "            # Split into training and calibration sets\n",
    "            X_train, X_cal, y_train, y_cal = train_test_split(nearest_neighbors, target, test_size=0.3, random_state=42)\n",
    "\n",
    "            if target_test is None:\n",
    "                continue\n",
    "            \n",
    "\n",
    "            start_time = time.time()\n",
    "            if method == 'Catboost':\n",
    "                \n",
    "                # categorical_features_indices = [nearest_neighbors.columns.get_loc(col) for col in processor.static_cat_cols+processor.dynamic_cat_cols]\n",
    "                # Create the CatBoostRegressor model\n",
    "                # model = CatBoostClassifier(iterations=100, loss_function='MultiClass', verbose=0, cat_features=processor.static_cat_cols + processor.dynamic_cat_cols) # , max_ctr_complexity=1\n",
    "                model = CatBoostClassifier(iterations=100, loss_function='Logloss', eval_metric='AUC', verbose=0, cat_features=[dataset_manager.activity_col]+dataset_manager.static_cat_cols)\n",
    "                print('Now training')\n",
    "                model.fit(X_train[features_used], y_train, cat_features=[dataset_manager.activity_col]+dataset_manager.static_cat_cols)\n",
    "\n",
    "            if method == 'HMM':\n",
    "                # Create an instance of the HMM model\n",
    "                model = hmm.GaussianHMM(n_components=7)  # Specify the number of hidden states\n",
    "                model.fit(nearest_neighbors[features_used])\n",
    "\n",
    "\n",
    "            if method == 'LogisticRegression':\n",
    "\n",
    "                preprocessor = make_column_transformer(\n",
    "                    (OneHotEncoder(handle_unknown=\"ignore\"), make_column_selector(dtype_include='object')),\n",
    "                    remainder='passthrough'\n",
    "                )\n",
    "                model = make_pipeline(\n",
    "                    preprocessor,\n",
    "                    LogisticRegression()\n",
    "                )\n",
    "                model.fit(X_train[features_used], y_train)\n",
    "\n",
    "            end_time = time.time()\n",
    "\n",
    "            training_time = (end_time - start_time) / 60\n",
    "            print(\"Training time: \", training_time, \" minutes\")\n",
    "            trainingtimes.append(training_time)\n",
    "\n",
    "            # Step 2: Wrap with calibration on held-out calibration data\n",
    "            calibrated_model = CalibratedClassifierCV(model, method='sigmoid', cv='prefit')\n",
    "            calibrated_model.fit(X_cal[features_used], y_cal)\n",
    "\n",
    "            # Make predictions on the testing data\n",
    "            print('Now predicting')\n",
    "            preds = model.predict(batch[features_used])\n",
    "            \n",
    "            # probs = model.predict_proba(batch[features_used])[:, 1]\n",
    "            # Step 3: Predict calibrated probabilities for the query instance\n",
    "            probs = calibrated_model.predict_proba(batch[features_used])[:, 1]\n",
    "            y_true = batch[dataset_manager.label_col].values\n",
    "            # Check if y_true contains at least two classes\n",
    "            if len(set(y_true)) > 1:\n",
    "                auc = roc_auc_score(y_true, probs)\n",
    "                AUCs.append(auc)\n",
    "            else:\n",
    "                print(\"Warning: Only one class present in y_true. Skipping ROC AUC calculation.\")\n",
    "                auc = None\n",
    "\n",
    "            cal_probs = calibrated_model.predict_proba(X_cal[features_used])[:, 1]\n",
    "            cal_true = X_cal[dataset_manager.label_col]\n",
    "            best_threshold, best_accuracy = find_threshold(cal_probs, cal_true)\n",
    "            preds_thr = probs > np.median(best_threshold)\n",
    "\n",
    "            # Check if array has null values\n",
    "            has_null = pd.Series(preds.flatten(), index=batch.index).isna().any()\n",
    "\n",
    "            if has_null:\n",
    "                print(\"Preds has null values\")\n",
    "\n",
    "\n",
    "            # true_conc_glu = row['Target_orig']\n",
    "            # preds_scaled = scalers[target_col].inverse_transform(preds.reshape(-1, 1))\n",
    "\n",
    "            batch.loc[:, 'predicted_value'] = pd.Series(preds.flatten(), index=batch.index)\n",
    "            batch.loc[:, 'proba_of_regular'] = pd.Series(probs.flatten(), index=batch.index)\n",
    "            batch.loc[:, 'predicted_thr'] = pd.Series(preds_thr.flatten(), index=batch.index)\n",
    "            batch.loc[:, 'predicted_thr'] = batch['predicted_thr'].map({False: 'deviant', True: 'regular'})\n",
    "            is_null = batch['predicted_value'].isna().any()\n",
    "            if is_null:\n",
    "                print(\"Batch has null values\")\n",
    "\n",
    "            results.append(batch)\n",
    "\n",
    "            # Add the current row with its prediction to the historic data\n",
    "            # row_with_prediction[target_col] = preds_scaled[0][0]\n",
    "            finished_case_ids = batch[batch['finished'] == 1][dataset_manager.case_id_col].unique()\n",
    "            finished_cases = current[current[dataset_manager.case_id_col].isin(finished_case_ids)]\n",
    "            historic = pd.concat([historic, finished_cases], ignore_index=True)\n",
    "            historic.sort_values([dataset_manager.case_id_col, dataset_manager.timestamp_col], ascending=True, kind='mergesort', inplace=True)\n",
    "            # historic.drop(columns=['predicted_value', 'proba_of_regular'], inplace=True)\n",
    "            # nn_model =processor.train_nn_model_bpic(historic)  # Refit the model with the updated historic data\n",
    "            historic_transformed = preprocessor.fit_transform(historic[features_used], historic[dataset_manager.label_col])\n",
    "            nn_model.fit(historic_transformed)\n",
    "            # num_model, cat_model = processor.train_nn_model_bpic_encoding(historic[features_used])\n",
    "\n",
    "        results_df = pd.concat(results)\n",
    "\n",
    "        # Calculate metrics\n",
    "        true_values = results_df[dataset_manager.label_col]\n",
    "        predicted_values = results_df['predicted_value']\n",
    "        proba_values = results_df['proba_of_regular']\n",
    "\n",
    "        accuracy = np.mean(true_values == predicted_values)\n",
    "        accuracy_thr = np.mean(true_values == results_df['predicted_thr'])\n",
    "        \n",
    "        # MAE_t = mean_absolute_error(true_values, predicted_values)\n",
    "        # MSE_t = mean_squared_error(true_values, predicted_values)\n",
    "        # RMSE_t = math.sqrt(MSE_t)\n",
    "        # r2_t = r2_score(true_values, predicted_values)\n",
    "        # mape_t = mean_absolute_percentage_error(true_values, predicted_values)\n",
    "\n",
    "        # Save results to a CSV file\n",
    "        # results_df.to_csv('predictions_with_row_data.csv', index=False)\n",
    "\n",
    "        # Print metrics\n",
    "        print(f\"Accuracy: {accuracy}\")\n",
    "        print(f\"Accuracy (Threshold): {accuracy_thr}\")\n",
    "        print(f\"f1_score: {f1_score(true_values, predicted_values, average='weighted')}\")\n",
    "        print(f\"Training Time: {sum(trainingtimes)/len(trainingtimes)}\")\n",
    "        print(f\"AUC: {sum(AUCs)/len(AUCs)}\")\n",
    "        # print(f\"MAE: {MAE_t}\")\n",
    "        # print(f\"MSE: {MSE_t}\")\n",
    "        # print(f\"RMSE: {RMSE_t}\")\n",
    "        # print(f\"R2: {r2_t}\")\n",
    "        # print(f\"MAPE: {mape_t}\")\n",
    "\n",
    "        output_dir = Path(f\"results/{dataset_name}\")\n",
    "        output_dir.mkdir(parents=True, exist_ok=True)\n",
    "        # dataset_name.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        results_df.to_csv(f'{output_dir}/{method}_{num_nearest_neighbors}_{distance_metric}.csv', index=False)\n",
    "        print('***********************************')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37391750",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix, auc, roc_curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "381e4ad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the distribution\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.hist(AUCs, bins=10, color='skyblue', edgecolor='black')\n",
    "plt.xlabel('AUC Values')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Distribution of AUC Values')\n",
    "plt.grid(axis='y', alpha=0.75)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8717562f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate FPR, TPR, and thresholds\n",
    "fpr, tpr, thresholds = roc_curve(dataset_manager.get_label_numeric(current), proba_values)\n",
    "\n",
    "# Calculate the AUC\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "# Plot the ROC curve\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(fpr, tpr, color='blue', lw=2, label=f'ROC curve (AUC = {roc_auc:.2f})')\n",
    "plt.plot([0, 1], [0, 1], color='gray', linestyle='--', lw=2)  # Diagonal line\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
    "plt.legend(loc='lower right')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3be6d97f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.classes_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92d86acc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c3e3252",
   "metadata": {},
   "outputs": [],
   "source": [
    "####### Baseline Version without JITL ########\n",
    "\n",
    "results_baseline = pd.DataFrame()\n",
    "\n",
    "print(f\"Experimenting with baseline version\")\n",
    "\n",
    "# Create the processor with the current values\n",
    "processor = processor_factory.get_processor(dataset_name, use_encoding=False, use_bucketing=False, num_nearest_neighbors=num_nearest_neighbors, distance_metric=distance_metric)\n",
    "\n",
    "\n",
    "\n",
    "data = merged_df.sort_values([dataset_manager.timestamp_col], ascending=True, kind='mergesort')\n",
    "# data['Target'] = data.groupby(case_id_col)['Target'].ffill().bfill()\n",
    "\n",
    "historic, current = processor.split_data_strict(data, train_ratio=0.5)\n",
    "historic.sort_values([dataset_manager.timestamp_col], ascending=True, kind='mergesort', inplace=True)\n",
    "current.sort_values([dataset_manager.timestamp_col], ascending=True, kind='mergesort', inplace=True)\n",
    "# current = current.head(1000)\n",
    "\n",
    "\n",
    "features_used = historic.columns.difference([dataset_manager.label_col, dataset_manager.timestamp_col, dataset_manager.case_id_col, 'case_length', 'event_nr', 'finished'])\n",
    "\n",
    "\n",
    "# # Encode the target column\n",
    "# label_encoder = LabelEncoder()\n",
    "# historic[dataset_manager.label_col] = label_encoder.fit_transform(historic[dataset_manager.label_col])\n",
    "# current[dataset_manager.label_col] = label_encoder.transform(current[dataset_manager.label_col])\n",
    "\n",
    "# # Update the target variables\n",
    "# target = historic[dataset_manager.label_col].values\n",
    "# target_test = current[dataset_manager.label_col].values\n",
    "\n",
    "\n",
    "target = historic[dataset_manager.label_col].values\n",
    "target_test = current[dataset_manager.label_col]\n",
    "\n",
    "start_time = time.time()\n",
    "if method == 'Catboost':\n",
    "        model = CatBoostClassifier(iterations=100, loss_function='Logloss', eval_metric='AUC', verbose=0, cat_features=[dataset_manager.activity_col]+dataset_manager.static_cat_cols)\n",
    "        print('Now training')\n",
    "        model.fit(historic[features_used], target, cat_features=[dataset_manager.activity_col]+dataset_manager.static_cat_cols)\n",
    "\n",
    "if method == 'HMM':\n",
    "    # Create an instance of the HMM model\n",
    "    model = hmm.GaussianHMM(n_components=7)  # Specify the number of hidden states\n",
    "    model.fit(historic[features_used])\n",
    "if method == 'LogisticRegression':\n",
    "    from sklearn.preprocessing import OneHotEncoder\n",
    "    from sklearn.pipeline import make_pipeline \n",
    "    #model = LogisticRegression()\n",
    "\n",
    "    from sklearn.compose import make_column_selector, make_column_transformer\n",
    "\n",
    "    preprocessor = make_column_transformer(\n",
    "        (OneHotEncoder(handle_unknown=\"ignore\"), make_column_selector(dtype_include='object')),\n",
    "        remainder='passthrough'\n",
    "    )\n",
    "    model = make_pipeline(\n",
    "        preprocessor,\n",
    "        LogisticRegression()\n",
    "    )\n",
    "    model.fit(X_train[features_used], y_train)\n",
    "\n",
    "end_time = time.time()\n",
    "training_time = (end_time - start_time) / 60\n",
    "print(\"Training time: \", training_time, \" minutes\")\n",
    "# trainingtimes.append(training_time)\n",
    "batch_size = 100\n",
    "\n",
    "print(\"Now predicting\")\n",
    "preds = model.predict(current[features_used])\n",
    "probs = model.predict_proba(current[features_used])[:, 1]\n",
    "y_true = current[dataset_manager.label_col].values\n",
    "auc = roc_auc_score(y_true, probs)\n",
    "\n",
    "results_baseline = current.copy()\n",
    "results_baseline.loc[:, 'predicted_value'] = pd.Series(preds.flatten(), index=current.index)\n",
    "results_baseline.loc[:, 'proba_of_regular'] = pd.Series(probs.flatten(), index=current.index)\n",
    "\n",
    "\n",
    "# for start in range(0, len(current), batch_size):\n",
    "#     end = start + batch_size\n",
    "#     batch = current.iloc[start:end]\n",
    "\n",
    "#     target = historic[dataset_manager.label_col].values\n",
    "#     target_test = batch[dataset_manager.label_col]\n",
    "\n",
    "#     if target_test is None:\n",
    "#         continue\n",
    "\n",
    "#     print('Now predicting')\n",
    "#     # Make predictions on the testing data\n",
    "#     preds = model.predict(batch[features_used])\n",
    "#     probs = model.predict_proba(batch[features_used])[:, 1]\n",
    "#     y_true = batch[dataset_manager.label_col].values\n",
    "#     if len(set(y_true)) > 1:\n",
    "#         auc = roc_auc_score(y_true, probs)\n",
    "#         AUCs.append(auc)\n",
    "#     else:\n",
    "#         print(\"Warning: Only one class present in y_true. Skipping ROC AUC calculation.\")\n",
    "#         auc = None\n",
    "\n",
    "#     # Check if array has null values\n",
    "#     has_null = pd.Series(preds.flatten(), index=batch.index).isna().any()\n",
    "#     if has_null:\n",
    "#         print(\"Preds has null values\")\n",
    "\n",
    "#     batch.loc[:, 'predicted_value'] = pd.Series(preds.flatten(), index=batch.index)\n",
    "#     is_null = batch['predicted_value'].isna().any()\n",
    "#     if is_null:\n",
    "#         print(\"Batch has null values\")\n",
    "\n",
    "#     results.append(batch)\n",
    "\n",
    "\n",
    "# results_df = pd.concat(results)\n",
    "\n",
    "# Calculate metrics\n",
    "true_values = results_baseline[dataset_manager.label_col]\n",
    "predicted_values = results_baseline['predicted_value']\n",
    "\n",
    "accuracy = np.mean(true_values == predicted_values)\n",
    "\n",
    "# Save results to a CSV file\n",
    "# results_df.to_csv('predictions_with_row_data.csv', index=False)\n",
    "\n",
    "# Print metrics\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(f\"f1_score: {f1_score(true_values, predicted_values, average='weighted')}\")\n",
    "print(f\"Training Time: {training_time}\")\n",
    "# print(f\"AUC: {sum(AUCs)/len(AUCs)}\")\n",
    "print(f\"AUC: {auc}\")\n",
    "# print(f\"MAE: {MAE_t}\")\n",
    "\n",
    "results_baseline.to_csv(f'results/{dataset_name}/baseline_{method}.csv', index=False)\n",
    "print('***********************************')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3ce5f94",
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = results_baseline.copy()\n",
    "tmp.sort_values([dataset_manager.timestamp_col], ascending=True, kind='mergesort', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f42ff38",
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp2 = results_df.copy()\n",
    "tmp2.sort_values([dataset_manager.timestamp_col], ascending=True, kind='mergesort', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19aa76ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_moving_avg_f1(df, true_col, pred_col, window_size=5):\n",
    "    # Sort the DataFrame by the timestamp column\n",
    "    df = df.sort_values([dataset_manager.timestamp_col], ascending=True, kind='mergesort')\n",
    "\n",
    "    # Convert columns to numpy arrays for faster operations\n",
    "    true_values = df[true_col].to_numpy()\n",
    "    predicted_values = df[pred_col].to_numpy()\n",
    "\n",
    "    # Preallocate arrays for results\n",
    "    num_rows = np.arange(2, len(true_values) + 1)  # Start from 2\n",
    "    f1_list = np.zeros(len(num_rows))\n",
    "\n",
    "    # Compute F1 scores incrementally\n",
    "    for i in range(2, len(true_values) + 1):\n",
    "        f1_list[i - 2] = f1_score(true_values[:i], predicted_values[:i], average='weighted')\n",
    "\n",
    "    # Create a DataFrame for results\n",
    "    f1_df = pd.DataFrame({'num_rows': num_rows, 'f1': f1_list})\n",
    "\n",
    "    # Compute the moving average of F1 scores\n",
    "    f1_df['moving_avg_f1'] = f1_df['f1'].rolling(window=window_size, min_periods=1).mean()\n",
    "\n",
    "    return f1_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d4092e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_moving_avg_acc_fast(df, true_col, pred_col, window_size=5):\n",
    "\n",
    "    df.sort_values([dataset_manager.timestamp_col], ascending=True, kind='mergesort', inplace=True)\n",
    "    # Convert columns to numpy arrays\n",
    "    true_values = df[true_col].to_numpy()\n",
    "    predicted_values = df[pred_col].to_numpy()\n",
    "\n",
    "\n",
    "    # Calculate cumulative accuracy (vectorized)\n",
    "    cumulative_correct = np.cumsum(true_values == predicted_values)\n",
    "    num_rows = np.arange(1, len(true_values) + 1)\n",
    "    cumulative_accuracy = cumulative_correct / num_rows\n",
    "\n",
    "    # Create DataFrame for results\n",
    "    mae_df = pd.DataFrame({\n",
    "        'num_rows': num_rows[1:],  # Start from 2\n",
    "        'mae': cumulative_accuracy[1:]  # Start from 2\n",
    "    })\n",
    "\n",
    "    # Calculate moving average of MAE\n",
    "    mae_df['moving_avg_mae'] = mae_df['mae'].rolling(window=window_size).mean()\n",
    "\n",
    "    return mae_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e279b48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate moving average MAE for tmp\n",
    "mae_df_tmp = calculate_moving_avg_acc_fast(tmp, dataset_manager.label_col, 'predicted_value')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "681ec469",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate moving average MAE for tmp2\n",
    "mae_df_tmp2 = calculate_moving_avg_acc_fast(tmp2, dataset_manager.label_col, 'predicted_value')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d090dc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Plot the moving average MAE for both DataFrames\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(mae_df_tmp['num_rows'], mae_df_tmp['moving_avg_mae'], label='baseline', color='blue')\n",
    "plt.plot(mae_df_tmp2['num_rows'], mae_df_tmp2['moving_avg_mae'], label='JIT-Cat', color='red')\n",
    "# plt.plot(mae_df_tmp3['num_rows'], mae_df_tmp3['moving_avg_mae'], label='euclidean', color='green')\n",
    "# plt.plot(mae_df_tmp4['num_rows'], mae_df_tmp4['moving_avg_mae'], label='DTW', color='black')\n",
    "plt.xlabel('Number of Observed Events over Time')\n",
    "plt.ylabel('Moving Average Accuracy')\n",
    "plt.title(f'{dataset_name} Moving Average Accuracy')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "# plt.show()\n",
    "plt.savefig(f\"results/{dataset_name}/moving_avg_accuracy_score.png\", dpi=600, bbox_inches='tight')\n",
    "plt.savefig(f\"results/{dataset_name}/moving_avg_accuracy_score.pdf\", dpi=600, bbox_inches='tight', format=\"pdf\")  # Save as PDF\n",
    "\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feee4a2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the moving average F1 score for both DataFrames\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(mae_df_tmp['num_rows'], mae_df_tmp['moving_avg_f1'], label='baseline', color='blue')\n",
    "plt.plot(mae_df_tmp2['num_rows'], mae_df_tmp2['moving_avg_f1'], label='JIT-Cat', color='red')\n",
    "# plt.plot(mae_df_tmp3['num_rows'], mae_df_tmp3['moving_avg_mae'], label='euclidean', color='green')\n",
    "# plt.plot(mae_df_tmp4['num_rows'], mae_df_tmp4['moving_avg_mae'], label='DTW', color='black')\n",
    "plt.xlabel('Number of Observed Events over Time')\n",
    "plt.ylabel('Moving Average F1 Score')\n",
    "plt.title(f'{dataset_name} Moving Average F1 Score')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "# plt.show()\n",
    "plt.savefig(f\"results/{dataset_name}/moving_avg_f1_score.png\", dpi=600, bbox_inches='tight')\n",
    "plt.savefig(f\"results/{dataset_name}/moving_avg_F1_score.pdf\", dpi=600, bbox_inches='tight', format=\"pdf\")  # Save as PDF\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4555c8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_results_to_csv(dataset_name, proposed_metrics, baseline_metrics, file_path='results_metrics.csv'):\n",
    "    \"\"\"\n",
    "    Save results metrics to a CSV file.\n",
    "\n",
    "    Parameters:\n",
    "        dataset_name (str): Name of the dataset.\n",
    "        proposed_metrics (dict): Metrics for the proposed method (keys: accuracy, f1_score, auc, precision, recall).\n",
    "        baseline_metrics (dict): Metrics for the baseline method (keys: accuracy, f1_score, auc, precision, recall).\n",
    "        file_path (str): Path to the results CSV file.\n",
    "    \"\"\"\n",
    "    # Define the columns for the CSV file\n",
    "    columns = [\n",
    "        'dataset_name',\n",
    "        'proposed_accuracy', 'proposed_f1_score', 'proposed_auc', 'proposed_precision', 'proposed_recall',\n",
    "        'baseline_accuracy', 'baseline_f1_score', 'baseline_auc', 'baseline_precision', 'baseline_recall'\n",
    "    ]\n",
    "\n",
    "    # Create a DataFrame for the new results\n",
    "    new_data = {\n",
    "        'dataset_name': dataset_name,\n",
    "        'proposed_accuracy': proposed_metrics['accuracy'],\n",
    "        'proposed_f1_score': proposed_metrics['f1_score'],\n",
    "        'proposed_auc': proposed_metrics['auc'],\n",
    "        'proposed_precision': proposed_metrics['precision'],\n",
    "        'proposed_recall': proposed_metrics['recall'],\n",
    "        'baseline_accuracy': baseline_metrics['accuracy'],\n",
    "        'baseline_f1_score': baseline_metrics['f1_score'],\n",
    "        'baseline_auc': baseline_metrics['auc'],\n",
    "        'baseline_precision': baseline_metrics['precision'],\n",
    "        'baseline_recall': baseline_metrics['recall']\n",
    "    }\n",
    "\n",
    "    new_row = pd.DataFrame([new_data])\n",
    "\n",
    "    # Check if the file exists\n",
    "    if not os.path.exists(file_path):\n",
    "        # If the file doesn't exist, create it with the appropriate headers\n",
    "        new_row.to_csv(file_path, index=False, columns=columns)\n",
    "    else:\n",
    "        # If the file exists, append the new row\n",
    "        existing_data = pd.read_csv(file_path)\n",
    "        # Check if the dataset already exists in the file\n",
    "        if dataset_name in existing_data['dataset_name'].values:\n",
    "            print(f\"Dataset '{dataset_name}' already exists in the results file. Updating the row.\")\n",
    "            \n",
    "            # Remove the existing row with the same dataset_name\n",
    "            existing_data = existing_data[existing_data['dataset_name'] != dataset_name]\n",
    "            \n",
    "            # Append the new row to the DataFrame\n",
    "            updated_data = pd.concat([existing_data, new_row], ignore_index=True)\n",
    "            \n",
    "            # Overwrite the file with the updated DataFrame\n",
    "            updated_data.to_csv(file_path, mode='w', index=False, columns=columns)\n",
    "        else:\n",
    "            new_row.to_csv(file_path, mode='a', index=False, header=False, columns=columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bfef0aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_metrics(results_df):\n",
    "    \"\"\"\n",
    "    Calculate metrics from a results DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "        results_df (pd.DataFrame): DataFrame containing 'true_labels', 'predicted_labels', and 'predicted_probs'.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary containing accuracy, f1_score, auc, precision, and recall.\n",
    "    \"\"\"\n",
    "    true_labels = results_df[dataset_manager.label_col]\n",
    "    predicted_labels = results_df['predicted_value']\n",
    "    predicted_probs = results_df['proba_of_regular']\n",
    "\n",
    "    metrics = {\n",
    "        'accuracy': accuracy_score(true_labels, predicted_labels),\n",
    "        'f1_score': f1_score(true_labels, predicted_labels, average='weighted'),\n",
    "        'auc': roc_auc_score(true_labels, predicted_probs),\n",
    "        'precision': precision_score(true_labels, predicted_labels, average='weighted'),\n",
    "        'recall': recall_score(true_labels, predicted_labels, average='weighted')\n",
    "    }\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "758141cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate metrics for proposed method\n",
    "proposed_metrics = calculate_metrics(results_df)\n",
    "\n",
    "# Calculate metrics for baseline method\n",
    "baseline_metrics = calculate_metrics(results_baseline)\n",
    "\n",
    "save_results_to_csv(f'{dataset_name}', proposed_metrics, baseline_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d19d0cb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
